\documentclass[a4paper,8pt]{article}
\usepackage[a4paper, margin=15mm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathfunc}

% Header
% ===========================

\title{Least Squares and Inner Products}
\author{Linear Algebra With Applications by Otto Bretscher (Chapter 5)}
\date{June 15\textsuperscript{th}, 2015}


% Document
% ===========================

\begin{document}
\maketitle
\pagenumbering{gobble}

\begin{outline}

  \tbullet{5.4.1 (Orthogonal Complements)}
    For any matrix \(A\), \((\text{im}(A))^{\perp} = \text{ker}(A^T)\).

    \begin{proof}
      Consider a subspace \(V = \text{im}(A)\) of \(\bbr^n\), where \(A = \left[\vec{v}_1\:\vec{v}_2\:\cdots\:\vec{v}_m\right]\). Then:
      \begin{align*}
        V^{\perp} &= \{ \vec{x}\text{ in }\bbr^n: \dotp{v}{x} = 0,\text{ for all } \vec{v} \text{ in } V \} \\
                  &= \{ \vec{x}\text{ in }\bbr^n: \vec{v}_i\cdot x = 0,\text{ for } i = 1,\ldots,m \} \\
                  &= \{ \vec{x}\text{ in }\bbr^n: \vec{v}_i^T\vec{x} = 0, \text{ for } i = 1, \ldots, m \}\text{.}
      \end{align*}
      Thus \(V^{\perp} = (\text{im}(A))^{\perp}\) is the kernel of the matrix
      \[ A^T = \begin{bmatrix}- & \vec{v}_1^T & - \\ - & \vec{v}_2^T & - \\ & \vdots & \\ - & \vec{v}_m^T & - \end{bmatrix}\text{.} \]
    \end{proof}

  \tbullet{5.4.2}
    \begin{enumerate}[i.]
      \item If \(A\) is an \(n \times m\) matrix, then \(\text{ker}(A) = \text{ker}(A^TA)\).
      \item If \(A\) is an \(n \times m\) matrix with \(\text{ker}(A) = \{\vec{0}\}\), then \(A^TA\) is invertible.
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          First note that \(\text{ker}(A) \subseteq \text{ker}(A^TA)\) since for all \(\vec{x}\in\text{ker}(A)\), \(A^TA\vec{x} = A^T\vec{0} = \vec{0}\).
          Now consider a vector \(\vec{x}\) in the kernel of \(A^TA\) such that \(A^TA\vec{x}=0\). Then \(A\vec{x}\) is in the image of \(A\) and in
          the kernel of \(A^T\). Since by Theorem 5.4.1 \(\text{ker}(A^T)\) is the orthogonal complement of \(\text{im}(A)\), \(A\vec{x} = \vec{0}\) by
          Theorem 5.1.10(ii). Therefore \(\vec{x}\) is in the kernel of \(A\).
        \item
          Note that \(A^TA\) is an \(m \times m\) matrix. By part (i), \(\text{ker}(A^TA) = \{\vec{0}\}\), and the square matrix \(A^TA\) is invertible.
      \end{enumerate}
    \end{proof}

  \tbullet{5.4.3}
    Consider a vector \(\vec{x}\) in \(\bbr^n\) and a subspace \(V\) of \(\bbr^n\). Then the orthogonal projection \(\text{proj}_V(\vec{x})\) is the
    vector in \(V\) closest to \(\vec{x}\), in that
    \[ \norm{\vec{x} - \text{proj}_V(\vec{x})} < \norm{\vec{x}-\vec{v}}\text{,} \]
    for all \(\vec{v}\) in \(V\) different from \(\text{proj}_V(\vec{x})\).

  \dbullet{5.4.4 (Least-Squares Solution)}
    Consider a linear system \(A\vec{x} = \vec{b}\), where \(A\) is an \(n \times m\) matrix. A vector \(\vec{x}^*\) in \(\bbr^m\) is called a
    "least-squares solution" of this system if \(\norm{\vec{b} - A\vec{x}^*} \leq \norm{\vec{b}-A\vec{x}}\) for all \(\vec{x}\) in \(\bbr^m\).

  \tbullet{5.4.5 (The Normal Equation)}
    The least-squares solutions of the system \(A\vec{x} = \vec{b}\) are the exact solutions of the (consistent) system
    \[ A^TA\vec{x} = A^T\vec{b}\text{.} \] This system is called the "normal equation" of \(A\vec{x} = \vec{b}\).

    \begin{proof}
      Suppose vector \(\vec{x}^*\) in a subspace \(V\) is a least-squares solution of the system \(A\vec{x} = \vec{b}\). Then by definition 5.4.4,
      \(\norm{\vec{b}-A\vec{x}^*} \leq \norm{\vec{b} - A\vec{x}}\) for all \(\vec{x}\) in \(\bbr^m\). By Theorem 5.4.3, \(\text{proj}_V\vec{b}\)
      satisfies this condition and so \(A\vec{x}^* = \text{proj}_V\vec{b}\). Thus the perpendicular component of \(A\vec{x}^*\), namely
      \(\vec{b}-A\vec{x}^*\), is in \(V^{\perp} = (\text{im}(A))^{\perp} = \text{ker}(A^T)\) by Theorem 5.4.1. Therefore
      \(A^T(\vec{b}-A\vec{x}^*) = \vec{0}\) and \(A^TA\vec{x}^* = A^T\vec{b}\).
    \end{proof}

  \tbullet{5.4.6 (Unique Least-Squares)}
    If \(\text{ker}(A) = \{\vec{0}\}\), then the linear system \(A\vec{x} = \vec{b}\) has the unique least-squares solution
    \[\vec{x}^* = (A^TA)^{-1}A^T\vec{b}\text{.}\]

    \begin{proof}
      If \(\text{ker}(A) = \{\vec{0}\}\), then by Theorem 5.4.2, \(A^TA\) is invertible. Solve for the normal equation accordingly.
    \end{proof}

  \tbullet{5.4.7 (Matrix of an Orthogonal Projection)}
    Consider a subspace \(V\) of \(\bbr^n\) with basis \(\inflatedot{\vec{v}}{m}\). Let
    \[ A = \begin{bmatrix} & & & \\ \vec{v}_1 & \vec{v}_2 & \quad & \vec{v}_m \\ & & & \end{bmatrix} \text{.} \]
    Then the matrix of the orthogonal projection onto \(V\) is \(A(A^TA)^{-1}A^T\).

    \begin{proof}
      Note that if \(\vec{x}^*\) is a least squares solution of the system \(A\vec{x} = \vec{b}\), then \(A\vec{x}^*\) is the orthogonal projection
      of \(\vec{b}\) onto \(\text{im}(A)\). Considering the columns of \(A\) are linearly independent, \(\text{ker}(A) = \{\vec{0}\}\) and so
      \(A^TA\) is invertible. Therefore \(\text{proj}_V\vec{b} = A\vec{x}^* = A(A^TA)^{-1}A^T\vec{b}\).
    \end{proof}

\end{outline}

\end{document}
