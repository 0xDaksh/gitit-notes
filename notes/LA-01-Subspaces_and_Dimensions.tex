\documentclass[a4paper,8pt]{article}
\usepackage[a4paper, margin=15mm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathfunc}

% Header
% ===========================

\title{Subspaces of \(\bbr^n\) and Their Dimensions}
\author{Linear Algebra With Applications by Otto Bretscher (Chapter 3)}
\date{May 29\textsuperscript{th}, 2015}


% Document
% ===========================

\begin{document}
\maketitle
\pagenumbering{gobble}

\begin{outline}

  % Section 3.1
  % ----------------------------------------------------------------------------

  \dbullet{3.1.1 (Image)}
    The "image" of a function consists of all the values the function takes in its target space. If \(f\) is a function
    from \(X\) to \(Y\), then \(\text{image}(f) = \{f(x): x \in X\} = \{ b \in Y : b = f(x) \text{ for some } x \in X\}\).

  \dbullet{3.1.2 (Span)}
    Consider the vectors \(\inflatedot{\vec{v}}{m}\in\bbr^n\). The set of all linear combinations \(c_1\vec{v}_1 +
    c_2\vec{v}_2 + \ldots, c_m\vec{v}_m\) of the vectors \(\inflatedot{\vec{v}}{m}\) is called their "span:"
    \(\text{span}(\inflatedot{\vec{v}}{m}) = \{c_1\vec{v}_1 + c_2\vec{v}_2 + \ldots + c_m\vec{v}_m :
    c_1, \ldots, c_m \in \bbr\}\).

  \tbullet{3.1.3 (Image and Span Relationship)}
    The image of a linear transformation is the span of the column vectors of \(A\). We denote the image of \(T\)
    by \(\text{im}(T)\) or \(\text{im}(A)\).

    \begin{proof}
      Let \(T(\vec{x}) = A\vec{x}\) where \(A = \begin{bmatrix} \vec{v}_1 & \ldots & \vec{v}_m \end{bmatrix}\). Then
      \(A\vec{x} = x_1\vec{v}_1 + \ldots + x_m\vec{v}_m\).
    \end{proof}

  \tbullet{3.1.4}
    The image of a linear transformation satisfy the following:
    \begin{enumerate}[i.]
      \item
        The zero vector \(\vec{0} \in \bbr^n\) is the image of \(T\).
      \item
        The image of \(T\) is "closed under addition:" if \(\vec{v}_1, \vec{v}_2 \in \text{im}(T)\), then
        \(\vec{v}_1 + \vec{v}_2 \in \text{im}(T)\).
      \item
        The image of \(T\) is "closed under scalar multiplication:" if \(\vec{v} \in \text{im}(T)\) and \(k\)
        is an arbitrary scalar, then \(k\vec{v} \in \text{im}(T)\).
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          \(\vec{0} = A\vec{0} = T(\vec{0})\).
        \item
          There exist \(\vec{w}_1, \vec{w}_2 \in \bbr^m\) such that \(T(\vec{w}_1) = \vec{v}_1\) and
          \(T(\vec{w}_2) = \vec{v}_2\). Then \(T(\vec{v}_1 + \vec{v}_2) = T(\vec{w}_1) + T(\vec{w}_2) =
          \vec{v}_1 + \vec{v}_2\).
        \item
          There exist \(\vec{w} \in \bbr^m\) such that \(T(\vec{w}) = \vec{v}\). Then \(T(k\vec{w}) =
          kT(\vec{w}) = k\vec{v}\) for some arbitrary scalar \(k\).
      \end{enumerate}
    \end{proof}

  \dbullet{3.1.5 (Kernel)}
    The "kernel" of a linear transformation \(T(\vec{x}) = A\vec{x}\) from \(\bbr^m\) to \(\bbr^n\)
    consists of all zeros of the transformation, that is, the solutions of the equation \(T(\vec{x}) = A\vec{x}
    = \vec{0}\).

  \tbullet{3.1.6}
    The kernel of a linear transformation satisfy the following:
    \begin{enumerate}[i.]
      \item
        The zero vector \(\vec{0}\) in \(\bbr^m\) is in the kernel of \(T\).
      \item
        The kernel is closed under addition and scalar multiplication.
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          \(T(\vec{0}) = A\vec{0} = \vec{0}\).
        \item
          Let \(\vec{v}_1, \vec{v}_2 \in \text{ker}(A)\), and \(k\) be an arbitrary scalar. Then \(T(\vec{v}_1
          + \vec{v}_2) = T(\vec{v}_1) + T(\vec{v}_2) = \vec{0} + \vec{0} = \vec{0}\). Similarly, \(T(k\vec{v})
          = kT(\vec{v}) = k\vec{0} = \vec{0}\).
      \end{enumerate}
    \end{proof}

  \tbullet{3.1.7}
    Let \(A\) be an \(n \times m\) matrix and \(B\) a square matrix. Then:
    \begin{enumerate}[i.]
      \item \(\text{ker}(A) = \{0\}\) if and only if \(\text{rank}(A) = m\).
      \item If \(\text{ker}(A) = \{0\}\), then \(m \leq n\). If \(m > n\), nonzero vectors are in \(\text{ker}(A)\).
      \item \(\text{ker}(B) = \{0\}\) if and only if \(B\) is invertible.
    \end{enumerate}


  % Section 3.2
  % ----------------------------------------------------------------------------

  \dbullet{3.2.1 (Linear Subspace)}
    A subset \(W\) of the vector space \(\bbr^n\) is called a "(linear) subspace of \(\bbr^n\)" if it
    has the following three properties:
    \begin{enumerate}[i.]
      \item \(W\) contains the zero vector in \(\bbr^n\),
      \item \(W\) is closed under addition and scalar multiplication
    \end{enumerate}

  \tbullet{3.2.2 (Kernel/Image Subspaces)}
    If \(T(\vec{x}) = A\vec{x}\) is a linear transformation from \(\bbr^m\) to \(\bbr^n\), then
    \(\text{ker}(T)\) is a subspace of \(\bbr^m\) and \(\text{image}(T)\) is a subspace of \(\bbr^n\).

  \dbullet{3.2.3 (Linear Independence)}
    Consider vectors \(\inflatedot{\vec{v}}{m} \in \bbr^n\). We say vector \(\vec{v}_i\) in list
    \(\inflatedot{\vec{v}}{m}\) is "redundant" if \(\vec{v}_i\) is a linear combination of preceding
    vectors \(\inflatedot{\vec{v}}{i}\). The vectors are "linearly independent" if none of them is
    redundant. Otherwise they are "linearly dependent." The vectors form a "basis" of subspace \(V\) of
    \(\bbr^n\) if they span \(V\) and are linearly independent (and that the vectors are in \(V\)).

  \tbullet{3.2.4 (Basis of Image)}
    To construct a basis of the image of a matrix \(A\), list all the column vectors of \(A\), and omit redundant
    vectors from the list.

  \tbullet{3.2.5 (Rule of Linear Independence)}
    Consider vectors \(\inflatedot{\vec{v}}{m} \in \bbr^n\). If \(\vec{v}_1\) is nonzero, and if each
    of the vectors \(\vec{v}_i, i \geq 2\), have a nonzero entry in a component where preceding vectors have \(0\),
    then they are linearly independent.

  \dbullet{3.2.6 (Relations)}
    Consider the vectors \(\inflatedot{\vec{v}}{m} \in \bbr^n\). An equation of the form \(c_1\vec{v}_1 +
    \ldots + c_m\vec{v}_m = \vec{0}\) is called a "(linear) relation" among the vectors. There is always the "trivial
    relation" \(c_1 = \ldots = c_m = 0\). "Nontrivial relations" may or may not exist.

  \tbullet{3.2.7}
    The vectors \(\inflatedot{\vec{v}}{m}\) in \(\bbr^n\) are linearly dependent if and only if there
    are nontrivial relations among them.

    \begin{proof}
      \forward
        Let \(\vec{v}_i = c_1\vec{v}_1 + \ldots + c_{i-1}\vec{v}_{i-1}\) be a redundant vector. Then we can
        generate a nontrivial relation \(\vec{0} = c_1\vec{v}_1 + \ldots + c_{i-1}\vec{v}_{i-1} - \vec{v}_i\)

      \backward
        Suppose a nontrivial relation \(c_1\vec{v}_1 + \ldots + c_i\vec{v}_i + \ldots + c_m\vec{v}_m = \vec{0}\)
        exists, where \(i\) is the highest index where \(c_i \neq 0\). Then \(\vec{v}_i = -\frac{c_1}{c_i}\vec{v}_1
        - \ldots - \frac{c_{i-1}}{c_i}\vec{v}_{i-1}\).
    \end{proof}

  \tbullet{3.2.8 (Max Linearly Independent Vectors)}
    The vectors in the kernel of an \(n \times m\) matrix \(A\) correspond to the linear relations among the column
    vectors of \(A\). The equation \(A\vec{x} = \vec{0} \Rightarrow x_1\vec{v}_1 + \ldots + x_m\vec{v}_m = \vec{0}\).
    In particular, the column vectors of \(A\) are linearly independent iff \(\text{ker}(A) = \{\vec{0}\}\), which
    implies \(m \leq n\). Thus we can find at most \(n\) linearly independent vectors in \(\bbr^n\).

    \begin{proof}
      Let \(\inflatedot{\vec{v}}{m}\) be the column vectors of an \(n \times m\) matrix \(A\). Then \(\text{ker}(A)
      = \text{ all } \vec{x} \text{ such that } A\vec{x} = x_1\vec{v}_1 + \ldots + x_m\vec{v}_m = \vec{0}\).

      \forward
        Suppose the column vectors are linearly independent. Then, by Theorem 3.2.7, only the trivial relation
        is a solution. Thus \(\text{ker}(A) = \{0\}\).

      \backward
        If \(\text{ker}(A) = \{0\}\), then only \(\vec{0}\) is a solution to \(A\vec{x} = \vec{0}\). Thus only
        the trivial relation works. Theorem 3.2.7 states the column vectors are linearly independent.

      By Theorem 3.1.7, \(\text{ker}(A) = \{0\} \Rightarrow m \leq n\).
    \end{proof}

  \tbullet{3.2.9}
    Consider the vectors \(\inflatedot{\vec{v}}{m}\) in a subspace \(V\) of \(\bbr^n\). The vectors form
    a basis of \(V\) if and only if every vector \(\vec{v} \in V\) can be expressed uniquely as a linear
    combination \(\vec{v} = c_1\vec{v}_1 + \ldots + c_m\vec{v}_m\).

    \begin{proof}
      \forward
        There exists at least one solution since \(\inflatedot{\vec{v}}{m}\) span \(V\). Suppose \(\vec{v} =
        c_1\vec{v}_1 + \ldots + c_m\vec{v}_m = d_1\vec{v}_1 + \ldots + d_m\vec{v}_m\). Then \(\vec{0} = (c_1-d_1)\vec{v}_1
        + \ldots + (c_m-d_m)\vec{v}_m\). Since the vectors are linearly independent, this must be the trivial relation,
        implying \(c_1 = d_1, \ldots, c_m = d_m\).

      \backward
        Suppose \(\vec{v} = c_1\vec{v}_1 + \ldots + c_m\vec{v}_m\) is the unique representation of \(\vec{v}\). Clearly
        the vectors span \(V\) since every \(\vec{v} \in V\) can be written as a linear combination of
        \(\inflatedot{\vec{v}}{m}\). Then this implies \(\vec{0} = c_1\vec{v}_1 + \ldots + c_m\vec{v}_m\) has a unique
        representation. Therefore only the trivial relation exists, and the vectors are linearly independent, satisfying
        the definition of a basis.
    \end{proof}


  % Section 3.3
  % ----------------------------------------------------------------------------

  \tbullet{3.3.1}
    Consider vectors \(\inflatedot{\vec{v}}{p}\) and \(\inflatedot{\vec{w}}{q}\) in a subspace \(V\)
    of \(\bbr^n\). If the vectors \(\inflatedot{\vec{v}}{p}\) are linearly independent, and vectors
    \(\inflatedot{\vec{w}}{q}\) span \(V\), then \(q \geq p\).

    \begin{proof}
      Consider matrices \(A = \begin{bmatrix} \vec{w}_1 & \ldots & \vec{w}_q \end{bmatrix}\) and
      \(B = \begin{bmatrix} \vec{v}_1 & \ldots & \vec{v}_p \end{bmatrix}\). Note that \(\text{im}(A) = V\)
      since the vectors \(\inflatedot{\vec{w}}{q}\) span \(V\). The vectors \(\inflatedot{\vec{v}}{p}\) are in
      this image and so \(\vec{v}_1 = A\vec{u}_1, \ldots, \vec{v}_p = A\vec{u}_p\) for
      some vectors \(\inflatedot{\vec{u}}{p}\) in \(\bbr^q\). Thus \(B \begin{bmatrix} \vec{v}_1
      & \ldots & \vec{v}_p \end{bmatrix} = A \begin{bmatrix} \vec{u}_1 & \ldots & \vec{u}_p \end{bmatrix} = C\),
      or \(B = AC\).

      The kernel of \(C\) is a subset of the kernel of \(B\) since \(C\vec{x} = \vec{0} \Rightarrow AC\vec{x} =
      A\vec{0} = \vec{0}\). But the kernel of \(B\) is \(\{0\}\) since \(\inflatedot{\vec{v}}{p}\) are
      linearly independent. Thus, \(\text{ker}(C) = \{0\}\) as well, which by Theorem 3.1.7, tells us \(C\) has
      as many rows as columns of \(q \geq p\).
    \end{proof}

  \tbullet{3.3.2 (Basis Equivalence)}
    All bases of a subspace \(V\) of \(\bbr^n\) consist of the same number of vectors.

    \begin{proof}
      Consider two bases \(\inflatedot{\vec{v}}{p}\) and \(\inflatedot{\vec{w}}{q}\) of \(V\).
      Then by Theorem 3.3.1, \(p \geq q\) and \(q \geq p\) so \(p = q\).
    \end{proof}

  \dbullet{3.3.3 (Dimension)}
    Consider a subspace \(V\) of \(\bbr^n\). The number of vectors in a basis of \(V\) is called
    the "dimension" of \(V\), denoted by \(\text{dim}(V)\).

  \tbullet{3.3.4 (Properties of Dimension)}
    Consider a subspace \(V\) of \(\bbr^n\) with \(\text{dim}(V) = m\).
    \begin{enumerate}[i.]
      \item We can find \textit{at most} \(m\) linearly independent vectors in \(V\).
      \item We need \textit{at least} \(m\) vectors to span \(V\).
      \item If \(m\) vectors in \(V\) are linearly independent, then they form a basis of \(V\).
      \item If \(m\) vectors in \(V\) span \(V\), then they form a basis of \(V\).
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          Consider linearly independent vectors \(\inflatedot{\vec{v}}{p}\) in \(V\), and let
          \(\inflatedot{\vec{w}}{m}\) be a basis of \(V\). Since the vectors \(\inflatedot{\vec{w}}{m}\)
          span \(V\), we have \(p \leq m\), by Theorem 3.3.1, as claimed.
        \item
          Let \(\inflatedot{\vec{v}}{m}\) be a basis of \(V\), and let \(\inflatedot{\vec{w}}{q}\) span \(V\).
          Since \(\inflatedot{\vec{v}}{m}\) forms a basis, they are linearly independent. Then by Theorem 3.3.1,
          \(q \geq m\).
        \item
          Consider linearly independent vectors \(\inflatedot{\vec{v}}{m}\) in \(V\). We
          have to show that the vectors \(\inflatedot{\vec{v}}{m}\) span \(V\). If \(\vec{v}\) is
          any vector in \(V\), then the \(m+1\) vectors \(\inflatedot{\vec{v}}{m}, \vec{v}\) will be
          linearly dependent by part (\romannumeral 1). Since vectors \(\inflatedot{\vec{v}}{m}\) are
          linearly independent and therefore nonredundant, vector \(\vec{v}\) must be redundant, meaning it is a linear
          combination of \(\inflatedot{\vec{v}}{m}\). Since \(\vec{v}\) is an arbitrary vector in \(V\),
          we have shown that vectors \(\inflatedot{\vec{v}}{m}\) span \(V\), as claimed.
        \item
          Consider vectors \(\inflatedot{\vec{v}}{m}\) that span \(V\), and let
          \(\inflatedot{\vec{w}}{m}\) be a basis of \(V\). We must show that \(\inflatedot{\vec{v}}{m}\) are linearly
          independent. For the sake of contradiction, suppose they weren't. Then for some \(\vec{v_i}\),
          \[\text{span}(\inflatedot{\vec{v}}{i}, \ldots, \vec{v_m}) = \text{span}(\inflatedot{\vec{v}}{m-1})\text{,}\]
          where the right hand side of the equation has been relabeled after the removal of \(\vec{v}_i\). But then we
          are saying \(m-1\) vectors span \(V\) and by Theorem 3.3.1, \(m-1 \geq m\) which is a contradiction.
          Thus \(\inflatedot{\vec{v}}{m}\) must be linearly independent, and therefore form a basis of \(V\).
      \end{enumerate}
    \end{proof}

  \tbullet{3.3.5}
    To construct a basis of the image of \(A\), pick the column vectors of \(A\) that correspond to the columns of
    \(\text{rref}(A)\) containing the leading \(1\)'s.

  \tbullet{3.3.6}
    For any matrix \(A\), \(\text{dim}(\text{im}(A)) = \text{rank}(A)\).

    \begin{proof}
      We note the nonredundant vectors of \(A\) correspond to the columns of \(\text{rref}(A)\) with leading \(1\)'s,
      as described in Theorem 3.3.5, which is equivalent to \(\text{rank}(A)\). These nonredundant vectors form
      \(\text{im}(A)\), of which the dimension is the number of linearly independent column vectors of \(A\).
    \end{proof}

  \tbullet{3.3.7 (Rank-Nullity Theorem)}
    For any \(n \times m\) matrix \(A\), the equation
    \[
      \text{dim}(\text{ker}(A)) + \text{dim}(\text{im}(A)) = m
    \]
    holds. The dimension of \(\text{ker}(A)\) is called the "nullity" of \(A\), and in Theorem 3.3.6 we observed
    that \(\text{dim}(\text{im}(A)) = \text{rank}(A)\).

  \tbullet{3.3.8}
    Suppose you are able to spot the redundant columns of a matrix \(A\). Express each redundant column as a linear
    combination of the preceding columns, \(\vec{v}_i = c_1\vec{v}_1 + \ldots + c_{i-1}\vec{v}_{i-1}\), write a
    corresponding relation, \(-c_1\vec{v}_1-\ldots-c_{i-1}\vec{v}_{i-1} + \vec{v}_i = \vec{0}\), and generate the vector
    \[\columnvec{\(-c_1\), \vdots, \(-c_{i-1}\), 1, 0, \vdots, 0}\] in the kernel of \(A\). The vectors so constructed
    form a basis of the kernel of \(A\). The nonredundant columns form a basis of the image of \(A\).

  \tbullet{3.3.9 (Basis and Invertibility)}
    The vectors \(\inflatedot{\vec{v}}{n} \in \bbr^n\) form a basis of \(\bbr^n\) if and only if the matrix
    \[
      \begin{bmatrix}
        \vert &  & \vert \\
        \vec{v}_1 & \ldots & \vec{v}_n \\
        \vert &  & \vert
      \end{bmatrix}
    \]
    is invertible.

    \begin{proof}
      Consider vectors \(\inflatedot{\vec{v}}{n}\). By Theorem 3.2.9, these form a basis of \(\bbr^n\) if and only
      if every vector \(\vec{b}\) in \(\bbr^n\) can be written uniquely as a linear combination of the vectors
      \(\inflatedot{\vec{v}}{n}\):
      \[
        \vec{b} = c_1\vec{v}_1 + \ldots + c_n\vec{v}_n =
        \begin{bmatrix} \vert &  & \vert \\ \vec{v}_1 & \ldots & \vec{v}_n \\ \vert &  & \vert \end{bmatrix}
        \columnvec{\(c_1\), \vdots, \(c_n\)}
      \]
      By definition of invertibility, the linear system
      \[
        \begin{bmatrix} \vert &  & \vert \\ \vec{v}_1 & \ldots & \vec{v}_n \\ \vert &  & \vert \end{bmatrix}
        \columnvec{\(c_1\), \vdots, \(c_n\)} = \vec{b}
      \]
      has a unique solution for all \(\vec{b}\) if and only if the \(n \times n\) matrix
      \[
        \begin{bmatrix} \vert &  & \vert \\ \vec{v}_1 & \ldots & \vec{v}_n \\ \vert &  & \vert \end{bmatrix}
      \]
      is invertible.
    \end{proof}


  % Section 3.4
  % ----------------------------------------------------------------------------

  \dbullet{3.4.1 (\(\beta\)-Coordinates)}
    Consider a basis \(\beta = (\inflatedot{\vec{v}}{m})\) of a subspace \(V\) of \(\bbr^n\). By Theorem 3.2.9,
    any vector \(\vec{x}\) in \(V\) can be written uniquely as \[ \vec{x} = c_1\vec{v}_1 + c_2\vec{v}_2 + \ldots +
    c_m\vec{v}_m\text{.} \] The scalars \(\inflatedot{c}{m}\) are called the "\(\beta\)-coordinates" of \(\vec{x}\),
    and the vector \[ \columnvec{\(c_1\), \(c_2\), \vdots, \(c_m\)} \] is the "\(\beta\)-coordinate vector" of
    \(\vec{x}\), denoted by \(\bcoor{\vec{x}}\). Thus \[ \bcoor{\vec{x}} = \columnvec{\(c_1\), \(c_2\),
    \vdots, \(c_m\)}\text{ means that } \vec{x} = c_1\vec{v}_1 + c_2\vec{v_2} + \ldots + c_m\vec{v}_m\text{.}\]

  \tbullet{3.4.2 (Linearity of Coordinates)}
    If \(\beta\) is a basis of a subspace \(V\) of \(\bbr^n\), then
    \begin{enumerate}[i.]
      \item
        \(\bcoor{\vec{x}+\vec{y}} = \bcoor{\vec{x}} + \bcoor{\vec{y}}\), for all vectors \(\vec{x}\)
        and \(\vec{y}\) in \(V\), and
      \item
        \(\bcoor{k\vec{x}} = k\bcoor{\vec{x}}\), for all \(\vec{x}\) in \(V\) and for all scalars \(k\).
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          Let \(\beta = (\inflatedot{\vec{v}}{m})\). If \(\vec{x} = c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_m\vec{v}_m\)
          and \(\vec{y} = d_1\vec{v}_1 + d_2\vec{v}_2 + \cdots + d_m\vec{v}_m\), then
          \begin{align*}
            \vec{x} + \vec{y} &= c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_m\vec{v}_m +
                                 d_1\vec{v}_1 + d_2\vec{v}_2 + \cdots + d_m\vec{v}_m \\
                              &= (c_1 + d_1)\vec{v}_1 + (c_2 + d_2)\vec{v}_2 + \cdots + (c_m + d_m)\vec{v}_m
          \end{align*}
          meaning
          \[
            \bcoor{\vec{x}+\vec{y}} = \columnvec{\(c_1+d_1\), \(c_2+d_2\), \vdots, \(c_m+d_m\)}
                                    = \columnvec{\(c_1\), \(c_2\), \vdots, \(c_m\)}
                                      + \columnvec{\(d_1\), \(d_2\), \vdots, \(d_m\)}
                                    = \bcoor{\vec{x}} + \bcoor{\vec{y}}
          \]
        \item
          Let \(\beta = (\inflatedot{\vec{v}}{m})\). If \(\vec{x} = c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_m\vec{v}_m\),
          then \(k\vec{x} = kc_1\vec{v}_1 + kc_2\vec{v}_2 + \cdots + c_m\vec{v}_m\), so that
          \[
            \left[k\vec{x}\right]_\beta
            = \columnvec{\(kc_1\), \(kc_2\), \vdots, \(kc_m\)}
            = k\columnvec{\(c_1\), \(c_2\), \vdots, \(c_m\)} = k\bcoor{\vec{x}}
          \]
          as claimed.
      \end{enumerate}
    \end{proof}

  \dbullet{3.4.3 (\(\beta\)-Matrices)}
    Consider a linear transformation \(T\) from \(\bbr^n\) to \(\bbr^n\) and a basis \(\beta\) of \(\bbr^n\).
    The \(n \times n\) matrix \(B\) that transforms \(\bcoor{\vec{x}}\) into \(\bcoor{T(\vec{x})}\) is called the
    "\(\beta\)-matrix of \(T\):" \[ \bcoor{T(\vec{x})} = B\bcoor{\vec{x}}\text{,} \] for all \(\vec{x}\) in \(\bbr^n\).

  \tbullet{3.4.4}
    We can construct a \(\beta\)-matrix \(B\) of a linear transformation \(T\) column by column as follows:
    \[ B = \begin{bmatrix} & & \\ \bcoor{T(\vec{v}_1)} & \cdots & \bcoor{T(\vec{v}_n)} \\ & & \end{bmatrix}\text{,} \] where
    \(\beta\) = \((\inflatedot{\vec{v}}{n})\).

    \begin{proof}
      Let \(\vec{x} = c_1\vec{v}_1 + \cdots + c_n\vec{v}_n\). By linearity of T and linearity of coordinates, we note
      \[T(\vec{x}) = c_1T(\vec{v}_1) + \cdots + c_nT(\vec{v}_n)\] and
      \begin{align*}
        \bcoor{T(\vec{x})} &= c_1\bcoor{T(\vec{v}_1)} + \cdots + c_n\bcoor{T(\vec{v}_n)} \\
                           &= \begin{bmatrix}
                                \bcoor{T(\vec{v}_1)} & \cdots & \bcoor{T(\vec{v}_n)}
                              \end{bmatrix}
                              \bcoor{\vec{x}}\text{.}
      \end{align*}
      Alternatively, we can note that to get the \(\spscript{i}{th}\) column of \(B\), we can find \(Be_i\), of which
      we then note \(\bcoor{\vec{v}_i} = e_i\). Since \(\bcoor{T(\vec{v}_i)} = B\bcoor{\vec{v}_i}\), the proof follows.
    \end{proof}

  \tbullet{3.4.5 (Change of Bases)}
    Consider a linear transformation \(T\) from \(\bbr^n\) to \(\bbr^n\) and a basis \(\beta = (\inflatedot{\vec{v}}{n})\)
    of \(\bbr^n\). Let \(B\) be the \(\beta\)-matrix of \(T\), and let \(A\) be the standard matrix of \(T\)
    (such that \(T(\vec{x}) = A\vec{x}\) for all \(\vec{x}\) in \(\bbr^n\)). Then
    \[
      AS=SB, B=S^{-1}AS,\text{ and }A=SBS^{-1},\text{ where }
      S = \begin{bmatrix} \vec{v}_1 &  \cdots & \vec{v}_n \end{bmatrix}\text{.}
    \]

    \begin{proof}
      Note \(T\) is a linear transformation such that \(A\vec{x} = T(\vec{x})\).
      Also note that for \(S=\begin{bmatrix} \vec{v}_1 & \cdots & \vec{v}_n \end{bmatrix}\),
      \[
        \vec{x} = S\bcoor{\vec{x}}\text{ and } T(\vec{x}) = S\bcoor{T(\vec{x})}\text{.}
      \]
      Thus \(T(\vec{x}) = AS\bcoor{\vec{x}}\) and \(T(\vec{x}) = SB\bcoor{\vec{x}}\) meaning \(AS=SB\).
    \end{proof}

  \dbullet{3.4.6 (Similar Matrices)}
    Consider two \(n\times n\) matrices \(A\) and \(B\). We say that \(A\) is "similar" to \(B\) if there exists an
    invertible matrix \(S\) such that \(AS = SB\). Thus two matrices are similar if they represent the same linear
    transformation with respect to different bases.

  \tbullet{3.4.7}
    Similarity is an equivalence relation.

    \begin{proof}
      Let \(A, B\), and \(C\) be similar \(n\times n\) matrices.

      \textbf{Reflexive}: Consider the identity matrix, an invertible \(n\times n\) matrix. Note \(AI_n = I_nA\) so
      \(A\) is similar to itself.

      \textbf{Symmetric}: Suppose there exists an invertible matrix \(S\) such that \(AS = SB\). Then
      \(B = S^{-1}AS \Rightarrow BS^{-1} = S^{-1}A\).

      \textbf{Transitive}: Suppose there exist invertible matrices \(P\) and \(Q\) such that \(AP = PB\) and \(BQ = QC\).
      Then \(AP = PQCQ^{-1} \Rightarrow APQ = PQC \Rightarrow AS = SC\) where \(S = PQ\) is invertible.

    \end{proof}

\end{outline}

\end{document}
