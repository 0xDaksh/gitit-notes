\documentclass[a4paper,8pt]{article}
\usepackage[a4paper, margin=15mm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathfunc}

% Header
% ===========================

\title{Random Variables}
\author{A First Course in Probability by Sheldon Ross (Chapter 4)}
\date{June 09\textsuperscript{th}, 2015}


% Document
% ===========================

\begin{document}
\maketitle
\pagenumbering{gobble}

\begin{outline}

  \dbullet{4.1.1 (Cumulative Distribution Function)}
    For a random variable \(X\), the function \(F\) defined by \[F(x)=\bbp[X\leq x], -\infty < x < \infty\]
    is called the "cumulative distributive function," or, more simply, the "distribution function," of \(X\).
    Thus, the distribution function specifies, for all real values \(x\), the probabiliity that the random
    variable is less than or equal to \(x\).

  \dbullet{4.2.1 (Discrete Random Variables)}
    A random variable that can take on at most a countable number of possible values is said to be "discrete."

  \dbullet{4.2.2 (Probability Mass Function)}
    For a discrete random variable \(X\), we define the "probability mass function" \(p(a)\) of \(X\) by
    \[p(a) = \bbp[X=a]\text{.}\] The probability mass function \(p(a)\) is positive for at most a countable
    number of values of \(a\). That is, if \(X\) must assume one of the values \(x_1, x_2, \ldots,\) then
    \begin{align*}
      p(x_i) &\geq 0\text{ for }i = 1, 2, \ldots \\
      p(x) &= 0\text{ for all other values of }x
    \end{align*}

  \dbullet{4.3.1 (Expected Value)}
    If \(X\) is a discrete random variable having a probability mass function \(p(x)\), then the "expectation," or the "expected value,"
    of \(X\), denoted by \(E[X]\), is defined by \[E[X] = \sum_{x:p(x)>0}xp(x)\text{.}\]

  \dbullet{4.3.2 (Indicator Variables)}
    We say that \(I\) is an "indicator variable" for the event \(A\) if
    \[
       I = \begin{cases}
             1 \text{ if } A \text{ occurs} \\
             0 \text{ if } A^c \text{ occurs}
       \end{cases}
    \]

  \pbullet{4.4.1 (Expectation of Function Compositions)}
    If \(X\) is a discrete random variable that takes on one of the values \(x_i, i\ge 1\), with respective
    probabilities \(p(x_i)\), then, for any real-valued function \(g\), \[ E[g(X)] = \sum_i g(x_i)p(x_i)\text{.} \]

    \begin{proof}
      We group together all terms in \(\sum_i g(x_i)p(x_i)\) with the same value of \(g(x_i)\). Specifically,
      suppose that \(y_j, j\geq 1\), represent the different values of \(g(x_i), i\geq 1\). Then, grouping
      all the \(g(x_i)\) having the same value gives
      \begin{align*}
        \sum_ig(x_i)p(x_i) &= \sum_j\sum_{i: g(x_i)=y_j} g(x_i)p(x_i) \\
                           &= \sum_j\sum_{i:g(x_i)=y_j} y_jp(x_i) \\
                           &= \sum_jy_j\sum_{i:g(x_i)=y_j}p(x_i) \\
                           &= \sum_jy_j\bbp[g(X)=y_j]\\
                           &= E[g(x)]
      \end{align*}
    \end{proof}

  \cbullet{4.4.2 (Expectation Factors)}
    If \(a\) and \(b\) are constants, then \(E[aX + b] = aE[X] + b\).

    \begin{proof}
      Letting \(X\) be a random variable, and \(a, b\) be constants:
      \begin{align*}
        E[aX + b] &= \sum_{x:p(x)>0}(ax + b)p(x) \\
                  &= a\sum_{x:p(x)>0}xp(x) + \sum_{x:p(x)>0}bp(x) \\
                  &= aE[X] + b
      \end{align*}
    \end{proof}

  \dbullet{4.4.3 (Moments of X)}
    The expected value of a random variable \(X, E[X]\), is also referred to as the "mean" or the "first moment
    of \(X\)." The quantity \(E[X^n], n\geq 1\), is called the "\(\spscript{n}{th}\) moment of \(X\)."

  \cbullet{4.4.4 (Expectation of Moments)}
    By Proposition 4.4.1, \[E[X^n] = \sum_{x:p(x)>0}x^np(x)\text{.}\]

  \dbullet{4.5.1 (Variance)}
    If \(X\) is a random variable with mean \(\mu\), then the variance of \(X\), denoted by \(\text{Var}(X)\)
    is defined by \[\text{Var}(X) = E[(X-\mu)^2]\text{.} \]

    \begin{justification}
      Because we expect \(X\) to take on values around \(E[X]\), a reasonable means of measuring variation would be
      \(E[|X-\mu]\), where \(\mu = E[X]\). This ends up being inconvenient to work with though, so the square of the
      difference between \(X\) and its mean is used instead.
    \end{justification}

  \tbullet{4.5.2 (Alternative Variance)}
    If \(X\) is a random variable with mean \(\mu\), then the variance of \(X\), denoted by \(\text{Var}(X)\)
    is defined by \[\text{Var}(X) = E[X^2]-(E[X])^2\text{.} \]

    \begin{proof}
      \begin{align*}
        \text{Var}(X) &= E[(X-\mu)^2] \\
                      &= \sum_x(x-\mu)^2p(x) \\
                      &= \sum_x(x^2-2\mu x+\mu^2)p(x) \\
                      &= \sum_x x^2p(x) - 2\mu\sum_x xp(x) + \mu^2\sum_x p(x) \\
                      &= E[X^2] - 2\mu^2 + \mu^2 \\
                      &= E[X^2] - \mu^2
      \end{align*}
    \end{proof}

  \cbullet{4.5.3 (Variance Factors)}
    For random variable \(X\) and constants \(a\) and \(b\),
    \[ \text{Var}(aX+b) = a^2\text{Var}(X)\text{.} \]

    \begin{proof}
      Let \(\mu = E[X]\) and note from Corollary 4.4.2 that \(E[aX+b] = a\mu+b\).
      Therefore
      \begin{align*}
        \text{Var}(aX+b) &= E[(aX+b-a\mu-b)^2] \\
                         &= E[a^2(X-\mu)^2] \\
                         &= a^2E[(X-\mu)^2] \\
                         &= a^2\text{Var}(X)
      \end{align*}
    \end{proof}

  \dbullet{4.5.4 (Standard Deviation)}
    The square root of the \(\text{Var}(X)\) is called the "standard deviation of \(X\)," and is denoted by
    \(\text{SD}(X)\). That is, \[\text{SD}(X) = \sqrt{\text{Var}(X)}\text{.}\]

  \pbullet{4.9.1 (Expectation via Outcomes)}
    For random variable \(X\), let \(X(s)\) denote the value of \(X\) when \(s \in S\) is the outcome of an experiment.
    Additionally, let \(p(s) = \bbp(\{s\})\) be the probability that \(s\) is the outcome of the experiment. Then
    \[ E[X] = \sum_{s\in S} X(s)p(s)\text{.} \]

    \begin{proof}
      Suppose that the distinct values of \(X\) are \(x_i, i \geq 1\). For each \(i\), let \(S_i\) be the event that
      \(X\) is equal to \(x_i\). That is, \(S_i = \{s: X(s) = x_i\}\). Then
      \begin{align*}
        E[X] &= \sum_i x_i\bbp[X=x_i] = \sum_i x_i\bbp[S_i] = \sum_i x_i \sum_{s \in S_i} p(s) \\
             &= \sum_i\sum_{s\in S_i} x_i p(s) = \sum_i\sum_{s\in S_i} X(s)p(s) \\
             &= \sum_{s\in S} X(s)p(s)
      \end{align*}
      where the final equality follows because \(S_1, S_2, \ldots\) are mutually exclusive events whose union is \(S\).
    \end{proof}

  \cbullet{4.9.2 (Linearity of Expectation)}
    For random variables \(\inflatedot{x}{n}\), \[ E\left[\sum_{i=1}^nX_i\right] = \sum_{i=1}^n E[X_i]\text{.} \]

    \begin{proof}
      Let \(Z = \sum_{i=1}^n X_i\). Then, by Proposition 4.9.1,
      \begin{align*}
        E[Z] &= \sum_{s\in S} Z(s)p(s) \\
             &= \sum_{s\in S} (X_1(s) + X_2(S) + \cdots + X_n(S))p(s) \\
             &= \sum_{s\in S} X_1(s)p(s) + \sum_{s\in S} X_2(s)p(s) + \cdots + \sum_{s\in S} X_n(s)p(s) \\
             &= E[X_1] + E[X_2] + \cdots + E[X_n]\text{.}
      \end{align*}
    \end{proof}

\end{outline}

\end{document}
