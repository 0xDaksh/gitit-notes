\documentclass[a4paper,8pt]{article}
\usepackage[a4paper, margin=15mm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathfunc}

% Header
% ===========================

\title{Appendix: Distributions}
\author{A First Course in Probability by Sheldon Ross (Chapter 4/5)}
\date{June \textsuperscript{th}, 2015}


% Document
% ===========================

\begin{document}
\maketitle
\pagenumbering{gobble}

\begin{outline}

  \dbullet{4.6.1 (Binomial Random Variables)}
    Let \(X\) be a random variable representing the number of successes occurring in \(n\) independent trials,
    each of which results in a success with probability \(p\) and failure with probability \(1-p\). Then
    \(X\) is said to be a "binomial random variable" with parameters \((n, p)\).

  \dbullet{4.6.2 (Bernoulli Random Variable)}
    A random variable \(X\) is said to be a "Bernoulli Random Variable" if its probability mass function is given by
    \begin{align*}
      p(0) &= \bbp[X=0] = 1 - p \\
      p(1) &= \bbp[X=1] = p
    \end{align*}
    where \(p\), \(0 \leq p \leq 1\), is the probability that a given trial is a success. Note a Bernoulli random
    variable can equivalently be identified as a binomial random variable with parameters \((1, p)\).

  \tbullet{4.6.3 (Binomial Distribution)}
    The probability mass function of a binomial random variable having parameters \((n, p)\) is given by
    \[ p(i) = \binom{n}{i}p^i(1-p)^{n-i}\quad i=0,1,\ldots,n\text{.} \]

  \tbullet{4.6.4 (Expectation of Binomial Random Variable)}
    The expectation of a binomial random variable \(X\) with parameters \(n\) and \(p\) is \(E[X] = np\).

  \tbullet{4.6.5 (Variance of Binomial Random Variable)}
    The variance of a binomial random variable \(X\) with parameters \(n\) and \(p\) is \(\text{Var}(X) = np(1-p)\).

  \pbullet{4.6.6}
    If \(X\) is a binomial random variable with parameters \((n, p)\), where \(0 < p < 1\), then as \(k\) goes
    from \(0\) to \(n\), \(\bbp[X=k]\) first increases monotonically and then decreases monotonically, reaching
    its largest value when \(k\) is the largest integer less than or equal to \((n+1)p\).

    \begin{proof}
      We prove the proposition by considering \(\bbp[X=k]/\bbp[X=k-1]\) and determining for what values of
      \(k\) it is greater or less than \(1\). Now,
      \begin{align}
        \frac{\bbp[X=k]}{\bbp[X=k-1]} &= \frac{\binom{n}{k}p^k(1-p)^{n-k}}
                                              {\binom{n}{k-1}p^{k-1}(1-p)^{n-k+1}}\nonumber \\
                                      &= \frac{(n-k+1)p}{k(1-p)}
      \end{align}
      Hence, \(\bbp[X=k] \geq \bbp[X=k-1]\) if and only if \((n-k+1)p \geq k(1-p)\), or, equivalently, if and only
      if \(k \leq (n+1)p\).
    \end{proof}

  \pbullet{4.6.7 (Computation of Binomial Distribution Function)}
    Supposing \(X\) is a binomial random variable with parameters \((n,p)\),
    \[ \bbp[X=k+1] = \frac{p}{1-p}\frac{n-k}{k+1}\bbp[X=k]\text{.} \]

    \begin{proof}
      From proof of Theorem 4.6.6 (1), we see the above follows.
    \end{proof}

  \dbullet{4.7.1 (Poisson Random Variable and Distribution)}
    A random variable \(X\) that takes on one of the values \(0, 1, 2, \ldots\) is said to be a
    "Poisson" random variable with parameter \(\lambda\) if, for some \(\lambda > 0\), \[ p(i) = \bbp[X=i]
    = e^{-\lambda}\frac{\lambda^i}{i!}\quad i=0,1,2,\ldots \]

  \tbullet{4.7.2 (Poisson Usage)}
      The Poisson random variable may be used as an approximation for a binomial random variable with
      parameters \((n,p)\) when \(n\) is large and \(p\) is small enough so that \(np\) is of moderate size.

      \begin{proof}
        Suppose \(X\) is a binomial random variable with parameters \((n,p)\), and let \(\lambda = np\). Then
        \begin{align*}
          \bbp[X=i] &= \binom{n}{i}p^i(1-p)^{n-i} \\
                    &= \binom{n}{i}\left(\frac{\lambda}{n}\right)^i\left(1-\frac{\lambda}{n}\right)^{n-i} \\
                    &= \frac{n(n-1)\cdots(n-i+1)}{n^i}\frac{\lambda^i}{i!}\frac{(1-\lambda/n)^n}{(1-\lambda/n)^i}\text{.}
        \end{align*}
        Now for \(n\) large and \(\lambda\) moderate
        \[
          \left(1-\frac{\lambda}{n}\right)^n \approx e^{-\lambda},\quad
          \frac{n(n-1)\cdots(n-i+1)}{n^i}\approx 1,\quad
          \left(1-\frac{\lambda}{n}\right)^i \approx 1\text{.}
        \]
        Where the first approximation derives from the definition of Euler's Number (SC.01.T3.6.4). Hence,
        \[\bbp[X=i] \approx e^{-\lambda}\frac{\lambda^i}{i!}\text{.} \]
      \end{proof}

  \tbullet{4.7.3 (Expectation of Poisson Variable)}
    Let \(X\) be a Poisson variable. Then \[E[X] = \lambda\text{.}\]

  \tbullet{4.7.4 (Variance of Poisson Variable)}
    Let \(X\) be a Poisson variable. Then \[\text{Var}(X) = \lambda\text{.}\]

  \pbullet{4.7.5 (Poisson Paradigm)}
    Consider \(n\) events, with \(p_i\) equal to the probability that event \(i\) occurs, \(i=1,\ldots,n\).
    If all the \(p_i\) are "small" and the trials are either independent or at most "weakly dependent,"
    then the number of these events that occur approximately has a Poisson distribution with mean \(\sum_{i=1}^np_i\).

  \pbullet{4.7.6 (Computation of Poisson Distribution Function)}
    If \(X\) is a Poisson with parameter \(\lambda\), then \[ \bbp[X=i+1] = \frac{\lambda}{i+1}\bbp[X=i]\text{.} \]

    \begin{proof}
      Given Poisson \(X\) with parameter \(\lambda\), then
      \[
        \frac{\bbp[X=i+1]}{\bbp[X=i]}
        = \frac{e^{-\lambda}\lambda^{i+1}/(i+1)!}{e^{-\lambda}\lambda^i/i!}
        = \frac{\lambda}{i+1}\text{.}
      \]
    \end{proof}

  \tbullet{4.8.1 (Geometric Distribution)}
    Suppose that independent trials, each having a probability \(p\), \(0 < p < 1\), of being a success, are performed
    until a success occurs. If we let \(X\) equal the number of trials required, then
    \[ \bbp[X=n] = (1-p)^{n-1}p,\quad n = 1,2,\ldots \]
    Any random variable \(X\) with this probability mass function is said to be a "geometric" random variable with parameter \(p\).

  \tbullet{4.8.2 (Expectation of Geometric Random Variable)}
    The expected value of a geometric random variable \(X\) with parameter \(p\) is \[E[X] = \frac{1}{p}\text{.}\]

  \tbullet{4.8.3 (Variance of Geometric Random Variable)}
    The variance of a geometric random variable \(X\) with parameter \(p\) is \[\text{Var}(X) = \frac{1-p}{p^2}\text{.}\]

  \tbullet{4.8.4 (Negative Binomial Distribution)}
    Suppose that independent trials, each having probability \(p\), \(0 < p < 1\), of being a success are performed until a
    total of \(r\) successes is accumulated. If we let \(X\) equal the number of trials required, then
    \[ \bbp[X=n] = \binom{n-1}{r-1}p^r(1-p)^{n-r},\quad n = r, r+1, \ldots \]
    A random variable \(X\) whose probability mass function is given by the above is said to be a "negative binomial" random
    variable with parameters \((r, p)\).

  \tbullet{4.8.5 (Expectation of Negative Binomial Distribution)}
    The expected value of a negative binomial random variable \(X\) with parameters \((r, p)\) is \[E[X] = \frac{r}{p}\text{.}\]

  \tbullet{4.8.6 (Variance of Negative Binomial Distribution)}
    The variance of a negative binomial random variable \(X\) with parameters \((r, p)\) is \[\text{Var}(X) = \frac{r(1-p)}{p^2}\text{.}\]

  \tbullet{4.8.7 (Hypergeometric Distribution)}
    Suppose that a sample of size \(n\) is to be chosen randomly (without replacement) from an urn containing \(N\) balls,
    of which \(m\) are white and \(N-m\) are black. If we let \(X\) denote the number of white balls selected, then
    \[ \bbp[X=i] = \frac{\binom{m}{i}\binom{N-m}{n-i}}{\binom{N}{n}},\quad i=0,1,\ldots,n\text{.} \]
    A random variable \(X\) whose probability mass function is given by the above for some values of \(n, N, m\) is said to
    by a "hypergeometric" random variable.

  \tbullet{4.8.8 (Expectation of Hypergeometric Distribution)}
    The expected value of a hypergeometric random variable \(X\) with parameters \((n, N, m)\) is \[E[X] = \frac{nm}{N}\text{.}\]

  \tbullet{4.8.9 (Variance of Hypergeometric Distribution)}
    The variance of a hypergeometric random variable \(X\) with parameters \((n, N, m)\) is
    \[\text{Var}(X) = np(1-p)\left(1 - \frac{n-1}{N-1}\right)\text{.}\]

\end{outline}

\end{document}
