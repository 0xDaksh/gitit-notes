\documentclass[a4paper,8pt]{article}
\usepackage[a4paper, margin=15mm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathfunc}

% Header
% ===========================

\title{Orthogonality and Least Squares}
\author{Linear Algebra With Applications by Otto Bretscher (Chapter 5)}
\date{June 10\textsuperscript{th}, 2015}


% Document
% ===========================

\begin{document}
\maketitle
\pagenumbering{gobble}

\begin{outline}

  \dbullet{5.1.1 (Orthogonality, Length, Unit Vectors)}
    \begin{enumerate}[i.]
      \item
        Two vectors \(\vec{v}\) and \(\vec{w}\) in \(\bbr^n\) are called "perpendicular" or "orthogonal"
        if \(\dotp{v}{w} = 0\).
      \item
        The "length" (or "magnitude" or "norm") of a vector \(\vec{v}\) in \(\bbr^n\) is
        \(\norm{\vec{v}} = \sqrt{\dotp{v}{v}}\).
      \item
        A vector \(\vec{u}\) in \(\bbr^n\) is called a "unit vector" if its length is \(1\).
    \end{enumerate}

  \dbullet{5.1.2 (Orthogonal to Subspaces)}
    A vector \(\vec{x}\) in \(\bbr^n\) is said to be orthogonal to a subspace \(V\) of \(\bbr^n\) if
    \(\vec{x}\) is orthogonal to all the vectors \(\vec{v}\) in \(V\), meaning that \(\dotp{x}{v} = 0\) for
    all vectors \(\vec{v}\) in \(V\).

  \dbullet{5.1.3 (Orthonormal Vectors)}
    The vectors \(\inflatedot{\vec{u}}{m}\) in \(\bbr^n\) are called "orthonormal" if they are all unit vectors and orthogonal to one another:
    \[
      \vec{u}_i \cdot \vec{u}_j = \begin{cases}
        1\text{ if }i = j \\
        0\text{ if }i \neq j
      \end{cases}
    \]

  \tbullet{5.1.4 (Properties of Orthonormal Vectors)}
    \begin{enumerate}[i.]
      \item Orthonormal vectors are linearly independent.
      \item Orthonormal vectors \(\inflatedot{\vec{u}}{n}\) in \(\bbr^n\) form a basis of \(\bbr^n\).
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          Consider a relation \[ c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_i\vec{u}_i + \cdots + c_m\vec{u}_m = \vec{0} \]
          among the orthonormal vectors \(\inflatedot{\vec{u}}{m}\) in \(\bbr^n\). Let us form the dot product of each side
          of this equation with \(\vec{u}_i\):
          \[
            (c_1\vec{u}_1+c_2\vec{u}_2+\cdots+c_i\vec{u}_i+\cdots+c_m\vec{u}_m)\cdot\vec{u}_i = \vec{0}\cdot\vec{u}_i = 0
            \text{.}
          \]
          Because the dot product is distributive,
          \[
            c_1(\vec{u}_1\cdot\vec{u}_i) + c_2(\vec{u}_2\cdot\vec{u}_i) + \cdots + c_i(\vec{u}_i\cdot\vec{u}_i) +
            \cdots+c_m(\vec{u}_m\cdot\vec{u}_i) = 0\text{.}
          \]
          We know that \(\vec{u}_i\cdot\vec{u}_i = 1\), and all other dot products are zero. Therefore \(c_i = 0\).
          Since this holds for all \(i = 1, \ldots, m\), it follows that the vectors \(\inflatedot{\vec{u}}{m}\) are
          linearly independent.
        \item
          From (\romannumeral 1), we note that the vectors \(\inflatedot{\vec{u}}{n}\) are linearly independent. We
          also note that any \(n\) linearly independent vectors in \(\bbr^n\) form a basis of \(\bbr^n\).
          The theorem follows.
      \end{enumerate}
    \end{proof}

  \tbullet{5.1.5 (Orthogonal Projection)}
    Consider a vector \(\vec{x}\) in \(\bbr^n\) and a subspace \(V\) of \(\bbr^n\). Then we can write
    \(\vec{x} = \vec{x}^{\parallel} + \vec{x}^{\perp}\), where \(\vec{x}^{\parallel}\) is in \(V\),
    \(\vec{x}^{\perp}\) is perpendicular to \(V\), and this representation is unique. The vector
    \(\vec{x}^{\parallel}\) is called the "orthogonal projection" of \(\vec{x}\) onto \(V\), denoted by
    \(\text{proj}_{V}(\vec{x})\).

    \begin{proof}
      Consider an "orthonormal" basis \(\inflatedot{\vec{u}}{m}\) of \(V\). If a decomposition
      \(\vec{x} = \vec{x}^{\parallel} + \vec{x}^{\perp}\) does exist, then we can write
      \[ \vec{x}^{\parallel} = c_1\vec{u}_1 + \cdots + c_i\vec{u}_i + \cdots + c_m\vec{u}_m\text{,} \]
      for some coefficients \(c_1, \ldots, c_i, \ldots, c_m\) yet to be determined (since \(\vec{x}^{\parallel}\)
      is in \(V\)).
      We know that
      \[
        \vec{x}^{\perp} = \vec{x}-\vec{x}^{\parallel} = \vec{x}-c_1\vec{u}_1 - \cdots - c_i\vec{u}_i - \cdots - c_m\vec{u}_m
      \]
      is orthogonal to \(V\), meaning that \(\vec{x}-c_1\vec{u}_1-\cdots c_i\vec{u}_i-\cdots-c_m\vec{u}_m\) is
      orthogonal to all the vectors \(\vec{u}_i\) in \(V\):
      \begin{align*}
        0 &= \vec{u}_i \cdot (\vec{x} - c_1\vec{u}_1 - \cdots - c_i\vec{u}_i - \cdots - c_m\vec{u}_m) \\
          &= \vec{u}_i\cdot x - c_1(\vec{u}_i\cdot\vec{u}_1)-\cdots-c_i(\vec{u}_i\cdot\vec{u}_i)-
             \cdots-c_m(\vec{u}_i\cdot\vec{u}_m) = \vec{u}_i\cdot\vec{x} - c_i\text{.}
      \end{align*}
      It follows that \(c_i = \vec{u}_i\cdot\vec{x}\), so that
      \[
        \vec{x}^{\parallel} = (\vec{u}_1\cdot\vec{x})\vec{u}_1 + \cdots
                            + (\vec{u}_i\cdot\vec{x})\vec{u}_i + \cdots
                            + (\vec{u}_m\cdot\vec{x})\vec{u}_m
      \] and
      \[
        \vec{x}^{\perp} = \vec{x} - \vec{x}^{\parallel}
                        = \vec{x} - (\vec{u}_1\cdot\vec{x})\vec{u}_1 - \cdots - (\vec{u}_i\cdot\vec{x})\vec{u}_i
                        - \cdots - (\vec{u}_m\cdot\vec{x})\vec{u}_m\text{.}
      \]
      Note that \(\vec{u}_i\cdot\vec{x}^{\perp} = 0\), by construction, so that \(\vec{x}^{\perp}\) is orthogonal
      to \(V\), as required.
    \end{proof}

  \tbullet{5.1.6 (Orthogonal Linearity)}
    The transformation \(T(\vec{x}) = \text{proj}_V(\vec{x}) = \vec{x}^{\parallel}\) from \(\bbr^n\) to \(\bbr^n\)
    is linear.

    \begin{proof}
      Consider transformation \(T(\vec{x}) = \text{proj}_V(\vec{x}) = \vec{x}^{\parallel}\) from \(\bbr^n\) to \(\bbr^n\).
      Let \(\vec{x}, \vec{y} \in \bbr^n\) and \(k\) be a scalar, and consider the following:
      \begin{enumerate}[i.]
        \item
            \(T(\vec{x}+\vec{y}) = \text{proj}_V(\vec{x}+\vec{y}) = (\vec{x}+\vec{y})^{\parallel}
                               = (\vec{x}+\vec{y}) - (\vec{x}+\vec{y})^{\perp}
                               = \vec{x} - \vec{x}^{\perp} + \vec{y} - \vec{y}^{\perp}
                               = T(\vec{x}) + T(\vec{y})\)
        \item
          \(T(k\vec{x}) = \text{proj}_V(k\vec{x}) = (k\vec{x})^{\parallel} = k(\vec{x}-\vec{x}^{\perp})
                        = k\vec{x}-k\vec{x}^{\perp} = k(\vec{x}-\vec{x}^{\perp}) = k\text{proj}_V(\vec{x})\).
      \end{enumerate}
      Therefore the transformation \(T(\vec{x})\) is linear.
    \end{proof}

  \tbullet{5.1.7 (Formula for Orthogonal Projections)}
    If \(V\) is a subspace of \(\bbr^n\) with an orthonormal basis \(\inflatedot{\vec{u}}{m}\), then
    \[ \text{proj}_V(\vec{x}) = \vec{x}^{\parallel} = (\vec{u}_1\cdot\vec{x})\vec{u}_1 + \cdots +
    (\vec{u}_m\cdot\vec{x})\vec{u}_m\text{,} \] for all \(\vec{x}\) in \(\bbr^n\).

  \tbullet{5.1.8}
    Consider an orthonormal basis \(\inflatedot{\vec{u}}{n}\) of \(\bbr^n\). Then \[ \vec{x} =
    (\vec{u}_1\cdot\vec{x})\vec{u}_1+\cdot+(\vec{u}_n\cdot\vec{x})\vec{u}_n\text{,} \] for all
    \(\vec{x}\) in \(\bbr^n\).

    \begin{proof}
      Apply Theorem 5.1.7 to the subspace \(V = \bbr^n\) of \(\bbr^n\) with orthonormal basis
      \(\inflatedot{\vec{u}}{n}\). Then \(\text{proj}_V\vec{x} = \vec{x}\), for all \(\vec{x}\) in \(V\).
      Therefore, by Theorem 5.1.5,
      \[
        \vec{x} = (\vec{u}_1\cdot\vec{x})\vec{u}_1+\cdots+(\vec{u}_n\cdot\vec{x})\vec{u}_n\text{,}
      \]
      for all \(\vec{x}\) in \(\bbr^n\).

      Alternatively, we can view Theorem 5.1.5 geometrically and note a projection of a subspace \(V\) of
      \(\bbr^n\) is the sum of projections of a given \(\vec{x}\) on each line spanned by the basis vectors
      \(\inflatedot{\vec{u}}{m}\) of \(V\). If we let \(V = \bbr^n\), this sum must be \(\vec{x}\).
    \end{proof}

  \dbullet{5.1.9 (Orthogonal Complement)}
    Consider a subspace \(V\) of \(\bbr^n\). The "orthogonal complement" \(V^{\perp}\) of \(V\) is the set of
    those vectors \(\vec{x}\) in \(\bbr^n\) that are orthogonal to all vectors in \(V\): \[ V^{\perp} =
    \{\vec{x}\text{ in }\bbr^n:\dotp{v}{x}=0\text{, for all }\vec{v}\text{ in }V\}\text{.} \]
    Note that \(V^{\perp}\) is the kernel of the orthogonal projection onto \(V\).

  \tbullet{5.1.10 (Properties of the Orthogonal Complement)}
    Consider a subspace \(V\) of \(\bbr^n\).
    \begin{enumerate}[i.]
      \item The orthogonal complement \(V^{\perp}\) of \(V\) is a subspace of \(\bbr^n\).
      \item The intersection of \(V\) and \(V^{\perp}\) consists of the zero vector alone:
            \(V \cap V^{\perp} = \{\vec{0}\}\).
      \item \(\text{dim}(V) + \text{dim}(V^{\perp}) = n\)
      \item \((V^{\perp})^{\perp} = V\)
    \end{enumerate}

    \begin{proof}
      Let \(V\) be a subspace of \(\bbr^n\).
      \begin{enumerate}[i.]
        \item
          If \(T(\vec{x}) = \text{proj}_V(\vec{x})\), then \(V^{\perp} = \text{ker}(T)\), a subspace of \(\bbr^n\).
        \item
          If a vector \(\vec{x}\) is in \(V\) as well as in \(V^{\perp}\), then \(\vec{x}\) is orthogonal to itself:
          \(\dotp{x}{x} = \norm{\vec{x}}^2 = 0\), so that \(\vec{x}\) must equal \(\vec{0}\), as claimed.
        \item
          We can apply the Rank-Nullity Theorem  to the linear transformation \(T(\vec{x}) = \text{proj}_V(\vec{x})\):
          \[ n = \text{dim}(\text{im}(T)) + \text{dim}(\text{ker}(T)) = \text{dim}(V) + \text{dim}(V^{\perp})\text{.} \]
        \item
          If \(T(\vec{x}) = \text{proj}_{V^{\perp}}(\vec{x})\), then \(\text{ker}(T) = (V^{\perp})^{\perp}\).
          Now note that for all \(\vec{v}\) in \(V\) and \(\vec{x}\) in \(V^{\perp}\), \(\dotp{v}{x} = 0\) by
          definition of an orthogonal complement.
          Then \(V \subseteq (V^{\perp})^{\perp}\) since \(\text{ker}(T) =
          \{ \vec{x}\text{ in }\bbr^n:\dotp{v}{x}=0\text{, for all }\vec{v}\text{ in }V^{\perp} \}\).
          By (\romannumeral 3) we note \[\text{dim}(V) + \text{dim}(V^{\perp}) = n \text{ and } \text{dim}(V^{\perp})
          + \text{dim}((V^{\perp})^{\perp}) = n\] implying \(\text{dim}(V) = \text{dim}((V^{\perp})^{\perp})\).
          Thus \(V = (V^{\perp})^{\perp}\).
      \end{enumerate}
    \end{proof}

  \tbullet{5.1.11 (Pythagorean Theorem)}
    Consider two vectors \(\vec{x}\) and \(\vec{y}\) in \(\bbr^n\). The equation \[\norm{\vec{x}+\vec{y}}^2 =
    \norm{\vec{x}}^2 + \norm{\vec{y}}^2 \] holds if and only if \(\vec{x}\) and \(\vec{y}\) are orthogonal.

    \begin{proof}
      For any two vectors \(\vec{x}, \vec{y} \in \bbr^n\):
      \begin{align*}
        \norm{\vec{x}+\vec{y}}^2 &= (\vec{x}+\vec{y})\cdot(\vec{x}+\vec{y}) \\
                                 &= \dotp{x}{x} + 2(\dotp{x}{y}) + \dotp{y}{y} \\
                                 &= \norm{\vec{x}}^2+2(\dotp{x}{y})+\norm{y} \\
                                 &= \norm{\vec{x}}^2+\norm{\vec{y}}^2\text{ if and only if }\dotp{x}{y} = 0\text{.}
      \end{align*}
    \end{proof}

  \tbullet{5.1.12}
    Consider a subspace \(V\) of \(\bbr^n\) and a vector \(\vec{x}\) in \(\bbr^n\). Then
    \[\norm{\text{proj}_V\vec{x}} \leq \norm{\vec{x}}\text{.}\] The statement is an equality if and only
    if \(\vec{x}\) is in \(V\).

    \begin{proof}
      By the Pythagorean theorem, \(\norm{\vec{x}}^2 = \norm{\text{proj}_V\vec{x}}^2 + \norm{\vec{x}^{\perp}}^2\).
      Thus \(\norm{\text{proj}_V\vec{x}} \leq \norm{\vec{x}}\), as claimed.
    \end{proof}

  \tbullet{5.1.13 (Cauchy-Schwarz Inequality)}
    If \(\vec{x}\) and \(\vec{y}\) are vectors in \(\bbr^n\), then \[|\dotp{x}{y}| \leq
    \norm{\vec{x}}\norm{\vec{y}}\text{.}\] This statement is an equality if and only if \(\vec{x}\)
    and \(\vec{y}\) are parallel.

    \begin{proof}
      Let \(V\) be a one-dimensional subspace of \(\bbr^n\) spanned by a nonzero vector \(\vec{y}\).
      We introduce the unit vector \[\vec{u} = \frac{1}{\norm{\vec{y}}}\vec{y}\] in \(V\). We know that
      \(\text{proj}_V\vec{x} = (\dotp{x}{u})\vec{u}\) for any \(\vec{x}\) in \(\bbr^n\). Then Theorem 5.1.12
      tells us that
      \[ \norm{\vec{x}}\geq\norm{\text{proj}_V(\vec{x})}
         = \norm{(\dotp{x}{u})\vec{u}}
         = |\dotp{x}{u}|=\left|\vec{x}\cdot\left(\frac{1}{\norm{\vec{y}}}\vec{y}\right)\right|
         = \frac{1}{\norm{\vec{y}}}|\dotp{x}{y}|\text{.}
      \]
    \end{proof}

  \dbullet{5.1.14 (Angle Between Two Vectors)}
    Consider two nonzero vectors \(x\) and \(y\) in \(\bbr^n\). The angle \(\theta\) between these vectors is
    defined as
    \[
      \theta = \arccos{\frac{\dotp{x}{y}}{\norm{\vec{x}}\norm{\vec{y}}}}\text{.}
    \]

    \begin{justification}
      We know the dot product of two nonzero vectors \(\vec{x}, \vec{y} \in \bbr^n\) is \(\dotp{x}{y} =
      \norm{\vec{x}}\norm{\vec{y}}\cos{\theta}\). Thus we solve for \(\theta\) for the above definition.
      Note that \(\theta\) is between \(0\) and \(\pi\) by definition of the inverse cosine function, and
      that we have to make sure \[ \arccos{\frac{\dotp{x}{y}}{\norm{\vec{x}}\norm{\vec{y}}}} \]
      is defined; that is, \[ \frac{\dotp{x}{y}}{\norm{\vec{x}}\norm{\vec{y}}} \] is between \(-1\) and \(1\).
      Fortunately
      \[
        \left|\frac{\dotp{x}{y}}{\norm{\vec{x}}\norm{\vec{y}}}\right|
        = \frac{|\dotp{x}{y}|}{\norm{\vec{x}}\norm{\vec{y}}} \leq 1
      \]
      by the Cauchy-Schwarz Inequality.
    \end{justification}

  \dbullet{5.1.15 (Correlations)}
    Consider two characteristics of a population, with deviation vectors \(\vec{x}\) and \(\vec{y}\).
    There is a "postive correlation" between the two characteristics if and only if \(\dotp{x}{y} > 0\).
    Similarly there is a "negative correlation" if and only if \(\dotp{x}{y} < 0\) and no correlation otherwise.

  \dbullet{5.1.16 (Correlation Coefficient)}
    The "correlation coefficient \(r\)" between two characteristics of a population is the cosine of the
    angle \(\theta\) between the deviation vectors \(\vec{x}\) and \(\vec{y}\) for the two characteristics:
    \[ r = \cos{\theta} = \frac{\dotp{x}{y}}{\norm{\vec{x}}\norm{\vec{y}}}\text{.} \]

  \tbullet{5.1.17 (Triangle Inequality)}
    Given vectors \(\vec{v}, \vec{w} \in \bbr^n\), the following holds: \[\norm{\vec{v}+\vec{w}}
    \leq \norm{\vec{v}} + \norm{\vec{w}}\text{.}\]

    \pagebreak
    \begin{proof}
      Note the following:
      \begin{align*}
        \norm{\vec{v}+\vec{w}}^2 &= (\vec{v}+\vec{w})\cdot(\vec{v}+\vec{w}) \\
                                 &= \dotp{v}{v} + \dotp{v}{w} + \dotp{w}{v} + \dotp{w}{w} \\
                                 &\leq \norm{\vec{v}}^2 + 2|\dotp{v}{w}| + \norm{\vec{w}}^2 \\
                                 &\leq \norm{\vec{v}}^2 + 2\norm{\vec{v}}\norm{\vec{w}} + \norm{\vec{w}}^2
                                  \text{ by Cauchy-Schwarz Inequality} \\
                                 &= (\norm{\vec{v}} + \norm{\vec{w}})^2
      \end{align*}
      Square rooting both sides yields the desired inequality.
    \end{proof}

  \tbullet{5.2.1 (Gram-Schmidt Process)}
    Consider a basis \(\inflatedot{\vec{v}}{m}\) of a subspace \(V\) of \(\bbr^n\). For \(j = 2, \ldots, m\),
    we resolve the vector \(\vec{v}_j\) into its components parallel and perpendicular to the span of the preceding
    vectors, \(\inflatedot{\vec{v}}{j-1}\):
    \[
      \vec{v}_j = \vec{v}_j^{\parallel} + \vec{v}_j^{\perp}, \text{ with respect to span}(\inflatedot{\vec{v}}{j-1})\text{.}
    \]
    Then
    \[
      \vec{u}_1 = \frac{1}{\norm{\vec{v}_1}}\vec{v}_1, \vec{u_2}
                = \frac{1}{\norm{\vec{v}_2^{\perp}}}, \ldots, \vec{u}_j
                = \frac{1}{\norm{\vec{v}_j^{\perp}}}\vec{v}_j^{\perp}, \ldots, \vec{u}_m
                = \frac{1}{\norm{\vec{v}_m^{\perp}}}\vec{v}_m^{\perp}
    \]
    is an orthonormal basis of \(V\). We also note
    \[
      \vec{v}_j^{\parallel} = \vec{v}_j - \vec{v}_j^{\perp}
                            = \vec{v}_j - (\vec{u}_1\cdot\vec{v}_j)\vec{u}_1 - \cdots
                            - (\vec{u}_{j-1}\cdot\vec{v}_j)\vec{u}_{j-1}\text{.}
    \]

  \tbullet{5.2.2 (QR Factorization)}
    Consider an \(n \times m\) matrix \(M\) with linearly independent columns \(\inflatedot{\vec{v}}{m}\).
    Then there exists an \(n \times m\) matrix \(Q\) whose columns \(\inflatedot{\vec{u}}{m}\) are orthonormal
    and an upper triangular matrix \(R\) with positive diagonal entries such that \(M = QR\). Then
    \(r_{11}=\norm{\vec{v}_1}, r_{jj} = \norm{\vec{v}_j^{\perp}}\) (for \(j = 2, \ldots, m\)),
    and \(r_{ij} = \vec{u}_i \cdot \vec{v}_j\) (for \(i < j\)).

    \begin{proof}
      The Gram-Schmidt Process represents a change of basis from basis \(\beta = (\inflatedot{\vec{v}}{m})\) to
      an orthonormal basis \(\mu = (\inflatedot{\vec{u}}{m})\) of \(V\). Thus, by Theorem 4.3.5,
      \[
        \begin{bmatrix} & & \\ \vec{v}_1 & \cdots & \vec{v}_m \\ & & \end{bmatrix} =
        \begin{bmatrix} & & \\ \vec{u}_1 & \cdots & \vec{u}_m \\ & & \end{bmatrix}R\text{,}
      \]
      where \(R\) is the change of basis matrix from \(\beta\) to \(\mu\) and the matrix before \(R\) is
      traditionally labelled \(Q\).

      By Theorem 4.3.4, the columns of \(R\) are the coordinates of \(\vec{v}_j\) with respect to basis \(\mu\). We note that
      \begin{align*}
        \vec{v}_j &= \vec{v}_j^{\parallel} + \vec{v}_j^{\perp} \\
                  &= (\vec{u}_1\cdot\vec{v}_j)\vec{u}_1 + \cdots + (\vec{u}_i\cdot\vec{v}_j)\vec{u}_i
                     + \cdots + (\vec{u}_{j-1}\cdot\vec{v}_j)\vec{u}_{j-1}
                     + \norm{\vec{v}_j^{\perp}}\vec{u}_j\text{.}
      \end{align*}
      It follows that \(r_{ij} = \vec{u}_i\cdot\vec{v}_j\) if \(i < j\), \(r_{jj} = \norm{\vec{v}_j^{\perp}}\),
      and \(r_{ij} = 0\) if \(i > j\). This in turn implies that \(R\) is upper triangle, with first diagonal
      entry \(r_{11} = \norm{\vec{v}_1},\) since \(\vec{v}_1 = \norm{\vec{v}_1}\vec{u}_1\).
    \end{proof}

  \tbullet{5.2.3 (QR Factorization Uniqueness)}
    Consider an \(n \times m\) matrix \(M\) with linearly independent columns \(\inflatedot{\vec{v}}{m}\).
    Then there exists an \(n \times m\) matrix \(Q\) whose columns \(\inflatedot{\vec{u}}{m}\) are orthonormal
    and an upper triangular matrix \(R\) with positive diagonal entries such that \(M = QR\). This representation
    is unique.

  \abullet{5.2.4 (Process of QR Factorization)}
    Consider an \(n \times m\) matrix \(M\) with linearly independent columns \(\inflatedot{\vec{v}}{m}\). Then the columns
    \(\inflatedot{\vec{u}}{m}\) of \(Q\) and the entries \(r_{ij}\) of \(R\) can be computed in the following order:
    \begin{gather*}
      r_{11} = \norm{\vec{v}_1}, \vec{u}_1 = \frac{1}{r_{11}}\vec{v_1} \\
      r_{12} = \vec{u}_1\cdot\vec{v}_2,\quad \vec{v}_2^{\perp} = \vec{v}_2-r_{12}\vec{u}_1,\quad
               r_{22} = \norm{\vec{v}_2^{\perp}},\quad \vec{u}_2 = \frac{1}{r_{22}}\vec{v}_2^{\perp},
    \end{gather*}
    and so on.

  \tbullet{5.3.1 (Orthogonal Transformations and Orthogonal Matrices)}
    A linear transformation  \(T\) from \(\bbr^n\) to \(\bbr^n\) is called "orthogonal" if it preserves the length of vectors:
    \[ \norm{T(\vec{x})} = \norm{\vec{x}},\text{ for all }\vec{x}\text{ in }\bbr^n\text{.} \]
    If \(T(\vec{x}) = A\vec{x}\) is an orthogonal transformation, we say \(A\) is an "orthogonal matrix."

  \tbullet{5.3.2 (Orthogonal Transformations Preserve Angles)}
    Consider an orthogonal transformation \(T\) from \(\bbr^n\) to \(\bbr^n\). If the vectors \(\vec{v}\) and \(\vec{w}\) in
    \(\bbr^n\) have a nonzero angle \(\theta\) between them, then so does \(L(\vec{v})\) and \(L(\vec{w})\).

    \begin{proof}
      Suppose \(L\) is an orthogonal transformation. Consider the angle \(\theta\) between vectors \(\vec{v}\) and \(\vec{w}\)
      in \(\bbr^n\). Then
      \[
        \measuredangle (L(\vec{v}), L(\vec{w}))
          = \arccos{\frac{L(\vec{v})\cdot L(\vec{w})}{\norm{L(\vec{v})}\norm{L(\vec{w})}}}
          = \arccos{\frac{\dotp{v}{w}}{\norm{\vec{v}}\norm{\vec{w}}}}
      \]
      where we note \(L(\vec{v})\cdot L(\vec{w}) = \dotp{v}{w}\) since
      \[
        \dotp{v}{w} = \norm{v}\norm{w}\cos{\theta} = \norm{L(\vec{v})}\norm{L(\vec{w})}\cos{\theta} = L(\vec{v})\cdot L(\vec{w})\text{.}
      \]

    \end{proof}

  \tbullet{5.3.3 (Orthogonal Transformations and Orthonormal Bases)}
    \begin{enumerate}[i.]
      \item
        A linear transformation \(T\) from \(\bbr^n\) to \(\bbr^n\) is orthogonal if and only if vectors \(T(\vec{e}_1),
        T(\vec{e}_2), \ldots, T(\vec{e}_n)\) form an orthonormal basis of \(\bbr^n\).
      \item
        An \(n\times n\) matrix \(A\) is orthogonal if and only if its columns form an orthonormal basis of \(\bbr^n\).
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          \forward If \(T\) is an orthogonal transformation, then, by definition, the \(T(\vec{e}_i)\) are unit vectors, and by
          Theorem 5.3.2, they are orthogonal.

          \backward Suppose the \(T(\vec{e}_i)\) form an orthonormal basis. Consider a vector \(\vec{x} = x_1\vec{e}_1 + x_2\vec{e}_2 + \cdots
          + x_n\vec{e}_n\) in \(\bbr^n\). Then
          \begin{align*}
            \norm{T(\vec{x})}^2 &= \norm{x_1T(\vec{e}_1) + x_2T(\vec{e}_2) + \cdots + x_nT(\vec{e}_n)}^2 \\
                                &= \norm{x_1T(\vec{e}_1)}^2 + \norm{x_2T(\vec{e}_2)}^2 + \cdots + \norm{x_nT(\vec{e}_n)}^2 \\
                                &= x_1^2 + x_2^2 + \cdots + x_n^2 \\
                                &= \norm{\vec{x}}^2\text{.}
          \end{align*}
        \item
          Proof follows from Theorem 2.1.2 which states the \(\spscript{i}{th}\) column of \(A\) can be formed by applying the transformation \(T\) on each
          \(\vec{e}_i\). Part (\romannumeral 1) then tells us that the columns must form an orthonormal basis for \(T\) to be orthogonal.
      \end{enumerate}
    \end{proof}

  \tbullet{5.3.4 (Products and Inverses of Orthogonal Matrices)}
    \begin{enumerate}[i.]
      \item The product \(AB\) of two orthogonal \(n \times n\) matrices \(A\) and \(B\) is orthogonal.
      \item The inverse \(A^{-1}\) of an orthogonal \(n \times n\) matrix \(A\) is orthogonal.
    \end{enumerate}

    \begin{proof}
      \item
        The linear transformation \(T(\vec{x}) = AB\vec{x}\) preserves length since
        \[ \norm{T(\vec{x})} = \norm{A(B\vec{x})} = \norm{B\vec{x}} = \norm{x}\text{.} \]
      \item
        The linear transformation \(T(\vec{x}) = A^{-1}\vec{x}\) preserves length since
        \[ \norm{A^{-1}\vec{x}} = \norm{A(A^{-1}\vec{x})} = \norm{AA^{-1}(\vec{x})} = \norm{\vec{x}}\text{.} \]
    \end{proof}

  \dbullet{5.3.5 (Transpose)}
    Consider an \(m \times n\) matrix \(A\). The "transpose" \(A^T\) of \(A\) is the \(n \times m\) matrix whose \(\spscript{ij}{th}\)
    entry is the \(\spscript{ji}{th}\) entry of \(A\): the roles of rows and columns are reversed.

  \dbullet{5.3.6 (Symmetric and Skew-Symmetric Matrices)}
    We say a square matrix \(A\) is "symmetric" if \(A^T = A\), and that \(A\) is "skew-symmetric" if \(A^T = -A\).

  \tbullet{5.3.7 (Alternative Dot Product)}
    If \(\vec{v}\) and \(\vec{w}\) are two column vectors in \(\bbr^n\), then \(\dotp{v}{w} = \vec{v}^T\vec{w}\).

  \tbullet{5.3.8 (Orthogonality Transpositions)}
    Consider an \(n \times n\) matrix \(A\). The matrix \(A\) is orthogonal if and only if \(A^TA = I_n\) or, equivalently,
    if \(A^{-1} = A^T\).

    \begin{proof}
      Let \(A\) be an \(n \times n\) matrix as follows:
      \[ A = \begin{bmatrix} \vert & \vert & & \vert \\ \vec{v}_1 & \vec{v}_2 & & \vec{v}_n \\ \vert & \vert & & \vert \end{bmatrix} \]
      Then
      \[
      A^TA = \begin{bmatrix}- & \vec{v}_1^T & - \\ - & \vec{v}_2^T & - \\ & \vdots & \\  - & \vec{v}_n^T & -\end{bmatrix}
             \begin{bmatrix} \vert & \vert & & \vert \\ \vec{v}_1 & \vec{v}_2 & & \vec{v}_n \\ \vert & \vert & & \vert \end{bmatrix}
           = \begin{bmatrix}
               \vec{v}_1\cdot\vec{v}_1 & \vec{v}_1\cdot\vec{v}_2 & & \vec{v}_1\cdot\vec{v}_n\\
               \vec{v}_2\cdot\vec{v}_1 & \vec{v}_2\cdot\vec{v}_2 & & \vec{v}_2\cdot\vec{v}_n\\
               \vdots & \vdots & & \vdots                                                   \\
               \vec{v}_n\cdot\vec{v}_1 & \vec{v}_n\cdot\vec{v}_2 & & \vec{v}_n\cdot\vec{v}_n
             \end{bmatrix}
             \text{.}
      \]
      By Theorem 5.3.3b, this product is \(I_n\) if and only if \(A\) is orthogonal.
    \end{proof}

  \tbullet{5.3.9 (Properties of the Transpose)}
    \begin{enumerate}[i.]
      \item If \(A\) is an \(n \times p\) matrix and \(B\) a \(p \times m\) matrix, then \((AB)^T = B^TA^T\).
      \item If an \(n \times n\) matrix \(A\) is invertible, then so is \(A^T\), and \((A^T)^{-1} = (A^{-1})^T\).
      \item For any matrix \(A\), \(\text{rank}(A) = \text{rank}(A^T)\).
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          We simply compare entries between the two:
          \begin{align*}
            \spscript{ij}{th}\text{ entry of }(AB)^T &= \spscript{ji}{th}\text{ entry of }AB \\
                                                     &= (\spscript{j}{th}\text{ row of }A)\cdot(\spscript{i}{th}\text{ column of } B)\\
            \spscript{ij}{th}\text{ entry of }B^TA^T &= (\spscript{i}{th}\text{ row of }B^T)\cdot(\spscript{j}{th}\text{ column of }A^T\\
                                                     &= (\spscript{i}{th}\text{ column of }B)\cdot(\spscript{j}{th}\text{ row of }A)\text{.}
          \end{align*}
        \item
          Note that \(AA^{-1} = I_n\). Transposing both sides and using part (\romannumeral 1), we find that
          \[ (AA^{-1})^T = (A^{-1})^TA^T = I_n\text{.} \] Multiplying both sides by \((A^T)^{-1}\) completes the proof.
        \item
          Consider the row space of \(A\) (i.e., the span of the rows of \(A\)). We note the dimension of this space is \(\text{rank}(A)\).
          Thus:
          \begin{align*}
            \text{rank}(A^T) &= \text{ dimension of the span of the columns of }A^T \\
                             &= \text{ dimension of the span of the rows of }A \\
                             &= \text{rank}(A)\text{.}
          \end{align*}
      \end{enumerate}
    \end{proof}

  \tbullet{5.3.10 (Matrix of an Orthogonal Projection)}
    Consider a subspace \(V\) of \(\bbr^n\) with orthonormal basis \(\inflatedot{\vec{u}}{m}\). The matrix of the orthogonal
    projection onto \(V\) is
    \[
      QQ^T,\quad\text{where }Q = \begin{bmatrix}
        \vert & \vert & & \vert \\
        \vec{u}_1 & \vec{u}_2 & \cdots & \vec{u}_m \\
        \vert & \vert & & \vert
      \end{bmatrix}\text{.}
    \]

    \begin{proof}
      Consider \[\text{proj}_V(\vec{x}) = (\vec{u}_1\cdot\vec{x})\vec{u}_1 + \cdots + (\vec{u}_m\cdot\vec{x})\vec{u}_m\]
      onto a subspace \(V\) of \(\bbr^n\) with orthonormal basis \(\inflatedot{\vec{u}}{m}\). We can write
      \begin{align*}
        \text{proj}_V\vec{x} &= \vec{u}_1\vec{u}_1^T\vec{x} + \cdots + \vec{u}_m\vec{u}_m^T\vec{x} \\
                             &= (\vec{u}_1\vec{u}_1^T + \cdots + \vec{u}_m\vec{u}_m^T)\vec{x} \\
                             &= \begin{bmatrix} \vert & & \vert \\ \vec{u}_1 & \cdots & \vec{u}_m \\ \vert & & \vert \end{bmatrix}
                                \begin{bmatrix} - & \vec{u}_1^T & - \\ & \vdots & \\ - & \vec{u}_m^T & - \end{bmatrix}\vec{x}\text{.}
      \end{align*}
    \end{proof}

\end{outline}

\end{document}
