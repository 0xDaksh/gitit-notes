\documentclass[a4paper,8pt]{article}
\usepackage[a4paper, margin=15mm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathfunc}

% Header
% ===========================

\title{Infinite Sequences and Series}
\author{Calculus Early Transcendentals by James Stewart (Chapter 11)}
\date{June 16\textsuperscript{th}, 2015}


% Document
% ===========================

\begin{document}
\maketitle
\pagenumbering{gobble}

\begin{outline}

  \dbullet{11.1.1 (Limits of Sequences)}
    A sequence \(\{a_n\}\) has the "limit" \(L\) and we write
    \[ \xlimit{n}{\infty}a_n = L \text{ or } a_n \rightarrow L\text{ as } n \rightarrow \infty \]
    if for every \(\epsilon > 0\), there is a corresponding integer \(N\) such that if \(n > N\)
    then \(|a_n - L| < \epsilon\).

  \tbullet{11.1.2}
    If \(\xlimit{x}{\infty} f(x) = L\) and \(f(n) = a_n\) when \(n\) is an integer, then \(\xlimit{n}{\infty} a_n = L\).

    \begin{proof}
      This merely follows from the definition of a limit in the continuous sense, but applied in a discrete sense.
    \end{proof}

  \tbullet{11.1.3 (Limit of Infinity)}
    The \(\xlimit{n}{\infty}a_n = \infty\) means that for every positive number \(M\) there is an integer \(N\) such that if \(n > N\)
    then \(a_n > M\).

  \tbullet{11.1.4 (Limit Laws)}
    If \(\{a_n\}\) and \(\{b_n\}\) are convergent sequences and \(c\) is a constant, then
    \begin{enumerate}
      \item \(\xlimit{n}{\infty} (a_n + b_n) = \xlimit{n}{\infty} a_n + \xlimit{n}{\infty} b_n\).
      \item \(\xlimit{n}{\infty} ca_n = c \xlimit{n}{\infty}a_n\).
      \item \(\xlimit{n}{\infty} (a_nb_n) = \xlimit{n}{\infty} a_n \cdot \xlimit{n}{\infty} b_n\).
      \item \(\xlimit{n}{\infty} (a_n/b_n) = \left(\xlimit{n}{\infty}a_n\right)/\left(\xlimit{n}{\infty}b_n\right)\) if \(\xlimit{n}{\infty}b_n \neq 0\).
      \item \(\xlimit{n}{\infty} a_n^p = \left[\xlimit{n}{\infty}a_n\right]^p\) if \(p > 0\) and \(a_n > 0\).
    \end{enumerate}

    \begin{proof}
      Proven in a similar manner to the limit laws of functions.
    \end{proof}

  \tbullet{11.1.5 (Squeeze Theorem)}
    If \(a_n \leq b_n \leq c_n\) for \(n \geq n_0\) and \(\xlimit{n}{\infty}a_n = \xlimit{n}{\infty} c_n = L\), then \(\xlimit{n}{\infty}b_n = L\).

    \begin{proof}
      Proof analogous to that of functions.
    \end{proof}

  \tbullet{11.1.6}
    If \(\xlimit{n}{\infty}|a_n| = 0\), then \(\xlimit{n}{\infty} a_n = 0\).

    \begin{proof}
      Let \(\epsilon > 0\). Then \(\exists N \in \bbn\) such that for all \(n \geq N\), \(||a_n|-0| < \epsilon\), meaning \(-\epsilon < |a_n| < \epsilon\).
      Observing the right inequality, we see \(|a_n| < \epsilon\) implies \(|a_n-0| < \epsilon\), completing the proof. Note the left inequality always
      holds true and can be disregarded.
    \end{proof}

  \tbullet{11.1.7}
    If \(\xlimit{n}{\infty} a_n = L\) and the function \(f\) is continuous at \(L\), then \[ \xlimit{n}{\infty} f(a_n) = f(L)\text{.} \]

    \begin{proof}
      Since \(f\) is continuous, we note the limit does exist, and the \(\delta-\epsilon\) definition holds; namely, \(\xlimit{x}{a} f(x) = f(L)\)
      means for every \(\epsilon > 0\), \(\exists \delta > 0\) such that \(0 < |x - a| < \delta\) implies \(|f(x)-f(L)| < \epsilon\). Now, let
      \(\epsilon > 0\) and take \(\delta > 0\) to be the value that satisfies the above definition. By hypothesis, there exists an \(N \in \bbn\) such
      that \(\forall n \geq N\), \(|a_n - L| < \delta\). But then \(|f(a_n) - f(L)| < \epsilon\) must also hold. Since \(\epsilon\) is arbitrary,
      \(\xlimit{n}{\infty} f(a_n) = f(L)\).
    \end{proof}

  \tbullet{11.1.8}
    The sequence \(\{r^n\}\) is convergent if \(-1 < r \leq 1\) and divergent for all other values of \(r\):
    \[
      \xlimit{n}{\infty} r^n = \begin{cases}
          0 &\text{ if } -1 < r < 1 \\
          1 &\text{ if } r = 1
        \end{cases}
    \]

  \dbullet{11.1.9 (Monotonicity)}
    A sequence \(\{a_n\}\) is called "increasing" if \(a_n < a_{n+1}\) for all \(n \geq 1\), that is \(a_1 < a_2 < \cdots\). It is called "decreasing"
    if \(a_n > a_{n+1}\) for all \(n \geq 1\). It is called "monotonic" if it is either increasing or decreasing.

  \dbullet{11.1.10 (Boundedness)}
    A sequence \(\{a_n\}\) is "bounded above" if there is a number \(M\) such that \(a_n \leq M\) for all \(n \geq 1\). It is "bounded below" if
    there is a number \(m\) such that \(m \leq a_n\) for all \(n \geq 1\). If it is bounded above and below, then \(\{a_n\}\) is a "bounded sequence."

  \tbullet{11.1.11 (Completeness Axiom)}
    For the set \(\bbr\) of real numbers, if \(S\) is a nonempty set of real numbers that has an upper bound \(M\), then \(S\) has a "least upper bound"
    or "supremum." Likewise, \(S\) also has a "least lower bound" or "infimum."

  \tbullet{11.1.12 (Monotonic Sequence Theorem)}
    Every bounded, monotonic sequence is convergent.

    \begin{proof}
      Suppose \(\{a_n\}\) is an increasing sequence. Since \(\{a_n\}\) is bounded, the set \(S = \{a_n : n \geq 1\}\) has an upper bound. By the Completeness
      Axiom, it has a supremum L. Given \(\epsilon > 0\), \(L - \epsilon\) is not an upper bound for \(S\). Therefore \(a_N > L - \epsilon\) for some
      integer \(N\). But since the sequence is increasing, \(a_n \geq a_N\) for every \(n > N\). Thus if \(n > N\) we have \(a_n > L-\epsilon\) which
      implies \(0 \leq L - a_n < \epsilon\) since \(a_n \leq L\). Therefore \(|L-a_n| < \epsilon\) whenever \(n > N\) and \(\xlimit{n}{\infty} a_n = L\).
      A similar proof holds when \(\{a_n\}\) is decreasing.
    \end{proof}

  \tbullet{11.2.1 (Convergence/Divergence of Series)}
    Given a series \(\sum_{n=1}^{\infty} a_n = a_1 + a_2 + \cdots\), let \(s_n\) denote its \(\spscript{n}{th}\) partial sum:
    \[ s_n = \sum_{i=1}^n a_i = \inflatedot[+]{a}{n}\text{.} \] If the sequence \(\{s_n\}\) is convergent and \(\xlimit{n}{\infty} s_n = s\)
    exists as a real number, then the series \(\sum a_n\) is called "convergent" and we write
    \[\inflatedot[+]{a}{n} + \cdots = s\text{ or } \sum_{n=1}^{\infty} a_n = s\text{.} \]
    The number \(s\) is called the "sum" of the series. Otherwise, the series is "divergent."

  \tbullet{11.2.2 (Geometric Series)}
    The geometric series \[ \sum_{n=1}^{\infty} ar^{n-1} = a + ar + ar^2 + \cdots \] is convergent if \(|r| < 1\) and its sum is
    \[ \sum_{n=1}^{\infty} ar^{n-1} = \frac{a}{1-r},\quad |r| < 1\text{.} \] If \(|r| \geq 1\), the geometric series is divergent.

    \begin{proof}
      We consider just the \(|r| < 1\) case, since \(r = 1\) and \(r > 1\) are obvious. Note that
      \begin{align*}
        s_n &= a + ar + ar^2 + \cdots + ar^{n-1} \\
        rs_n &= ar + ar^2 + \cdots + ar^{n-1} + ar^n\text{.}
      \end{align*}
      Subtracting the two equations, we get
      \begin{align*}
        s_n - rs_n &= a - ar^n \\
        s_n &= \frac{a(1-r^n)}{1-r}
      \end{align*}
      By Theorem 11.1.8, \(r^n \rightarrow \infty\) as \(n \rightarrow \infty\), and thus \(s_n = a/(1-r)\).
    \end{proof}

  \tbullet{11.2.3}
    If the series \(\sum_{n=1}^{\infty}\) is convergent, then \(\xlimit{n}{\infty} = 0\).

    \begin{proof}
      Let \(s_n = \inflatedot[+]{a}{n}\). Then \(a_n = s_n - s_{n-1}\). Since \(\sum a_n\) is convergent, the sequence \(\{s_n\}\) is convergent.
      Let \(\xlimit{n}{\infty} s_n = s\). Since \(n - 1 \rightarrow \infty\) as \(n \rightarrow \infty\), we also have \(\xlimit{n}{\infty} s_{n-1} = s\).
      Therefore
      \begin{align*}
        \xlimit{n}{\infty} a_n &= \xlimit{n}{\infty}(s_n - s_{n-1}) = \xlimit{n}{\infty} s_n - \xlimit{n}{\infty} s_{n-1} \\
                               &= s - s = 0\text{.}
      \end{align*}
    \end{proof}

  \tbullet{11.2.4 (Divergence Test)}
    If \(\xlimit{n}{\infty} a_n\) does not exist or if \(\xlimit{n}{\infty} a_n \neq 0\), then the series \(\sum_{n=1}^{\infty} a_n\) is divergent.

    \begin{proof}
      Contrapositive of Theorem 11.2.3.
    \end{proof}

  \tbullet{11.3.1 (The Integral Test)}
    Suppose \(f\) is a continuous, positive, decreasing function on \([1, \infty)\) and let \(a_n = f(n)\). Then the series \(\sum_{n=1}^{\infty} a_n\)
    is convergent if and only if the improper integral \(\int_1^{\infty}f(x)dx\) is convergent. In other words:
    \begin{enumerate}[i.]
      \item If \(\int_1^{\infty}f(x)dx\) is convergent, then \(\sum_{n=1}^{\infty} a_n\) is convergent.
      \item If \(\int_1^{\infty}f(x)dx\) is divergent, then \(\sum_{n=1}^{\infty} a_n\) is divergent.
    \end{enumerate}

  \tbullet{11.3.2 (Remainder Estimate for Integral Test)}
    Suppose \(f(k) = a_k\), where \(f\) is a continuous, positive, decreasing function for \(x \geq n\) and \(\sum a_n\) is convergent.If
    \(R_n = s - s_n\), then \[ \sum_{n+1}^{\infty} f(x)dx \leq R_n \leq \sum_n^{\infty} f(x)dx\text{.} \]

    \begin{proof}
      Consider any partial sum \(s_n\) and note it is an approximation to \(s\) since \(\xlimit{n}{\infty}s_n = s\). Consider the remainder:
      \[ R_n = s - s_n = a_{n+1} + a_{n+2} + \cdots \]
      Assuming that \(f\) is a continuous decreasing function on \([n, \infty)\), we can take the left and right Riemann sums under \(y = f(x)\)
      respectively:
      \begin{align*}
        R_n &= a_{n+1} + a_{n+2} + \cdots \leq \int_n^{\infty} f(x)dx \\
        R_n &= a_{n+1} + a_{n+2} + \cdots \geq \int_{n+1}^{\infty} f(x)dx
      \end{align*}
    \end{proof}

  \cbullet{11.3.3 (Summation Approximation)}
    Suppose \(f(k) = a_k\), where \(f\) is a continuous, positive, decreasing function for \(x \geq n\) and \(\sum a_n\) is convergent.If \(s_n\)
    is the partial sum up to \(n\) elements, then:
    \[ s_n + \int_{n+1}^{\infty} f(x)dx \leq s \leq s_n + \int_n^{\infty} f(x)dx\text{.} \]

    \begin{proof}
      This follows from Theorem 11.3.2 and the fact \(R_n = s - s_n\) for some partial sum \(s_n\).
    \end{proof}

  \tbullet{11.4.1 (Comparison Test)}
    Suppose that \(\sum a_n\) and \(\sum b_n\) are series with positive terms. Then
    \begin{enumerate}[i.]
      \item If \(\sum b_n\) is convergent and \(a_n \leq b_n\) for all \(n\). then \(\sum a_n\) is also convergent.
      \item If \(\sum b_n\) is divergent and \(a_n \geq b_n\) for all \(n\), then \(\sum a_n\) is also divergent.
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          Let \(s_n = \sum_{i=1}^n a_i\), \(t_n = \sum_{i=1}^n b_i\), and \(t = \sum_{n=1}^{\infty} b_n\).
          Since both series have positive terms, the sequences \(\{s_n\}\) and \(\{t_n\}\) are increasing. Also \(t_n \rightarrow t\),
          so \(t_n \leq t\) for all \(n\). Since \(a_i \leq b_i\), we have \(s_n \leq t_n\). Thus \(s_n \leq t\) for all \(n\). This
          means that \(\{s_n\}\) is increasing and bounded above and therefore converges by the Monotonic Sequence Theorem. Thus
          \(\sum a_n\) converges.
        \item
          If \(\sum b_n\) is divergent, then \(t_n \rightarrow \infty\). But \(a_i \geq b_i\) so \(s_n \geq t_n\). Thus
          \(s_n \rightarrow \infty\). Therefore \(\sum a_n\) diverges.
      \end{enumerate}
    \end{proof}

  \tbullet{11.4.2 (Limit Comparison Test)}
    Suppose that \(\sum a_n\) and \(\sum b_n\) are series with positive terms. If
    \[ \xlimit{n}{\infty} \frac{a_n}{b_n} = c \] where \(c\) is a finite number and \(c > 0\), then either both
    series converge or both diverge.

    \begin{proof}
      Let \(m\) and \(M\) be positive numbers such that \(m < c < M\). Because \(a_n/b_n\) is close to \(c\) for large \(n\),
      there is an integer \(N\) such that \[ m < \frac{a_n}{b_n} < M,\quad \text{when }n > N \] and so
      \[ mb_n < a_n < Mb_n,\quad \text{when }n > N\text{.} \] If \(\sum b_n\) converges, so does \(\sum Mb_n\). Thus \(\sum a_n\)
      converges by part (i) of the Comparison Test. If \(\sum b_n\) diverges, so does \(\sum mb_n\) and part (ii) of the Comparison
      Test shows that \(\sum a_n\) diverges.
    \end{proof}

  \tbullet{11.5.1 (The Alternating Series Test)}
    If the alternating series \[ \sum_{n=1}^{\infty} (-1)^{n-1} b_n = b_1 - b_2 + b_3 - b_4 + \cdots, b_n > 0 \] satisfies
    \(b_{n+1} \leq b_n\) for all \(n\) and \(\xlimit{n}{\infty}b_n = 0\), then the series is convergent.

    \begin{proof}
      We first consider the even partial sums, noting that \(s_2 = b_1 - b_2 \geq 0\), \(s_4 = s_2 + (b_3 - b_4) \geq s_2\), and, in general,
      \[ s_{2n} = s_{2n-2} + (b_{2n-1} - b_{2n}) \geq s_{2n-2}\text{.} \] Thus \( 0 \leq s_2 \leq s_4 \leq \cdots \leq s_{2n} \leq \cdots\)
      But we also note \(s_{2n} = b_1 - (b_2 - b_3) - (b_4 - b_5) - \cdots - (b_{2n-2} - b_{2n-1}) - b_{2n}\).
      Every term in parenthesis is positive so \(s_{2n} \leq b_1\) for all \(n\). Thus the sequence \(\{s_{2n}\}\) of even partial sums
      is increasing, bounded above, and by the Monotonic Sequence Theorem, convergent to a limit \(s\).

      Considering the limit of the odd partial sums:
      \begin{align*}
        \xlimit{n}{\infty} s_{2n+1} &= \xlimit{n}{\infty} (s_{2n} + b_{2n+1}) \\
                                    &= \xlimit{n}{\infty} s_{2n} + \xlimit{n}{\infty} b_{2n+1} \\
                                    &= s + 0 = s\text{.}
      \end{align*}
      Since both even and odd partial sums convege to \(s\), we have \(\xlimit{n}{\infty} s_n = s\), and so the series is convergent.
    \end{proof}

  \tbullet{11.5.2 (Alternating Series Estimation Theorem)}
    If \(s = \sum (-1)^{n-1}b_n\) is the sum of an alternating series that satisfies \(0 \leq b_{n+1} \leq b_n\) and \(\xlimit{n}{\infty} b_n = 0\),
    then \[ |R_n| = |s - s_n| \leq b_{n+1}\text{.} \]

    \begin{proof}
      We know from the proof of the Alternating Series Test that \(s\) lies between any two consecutive partial sums \(s_n\) and \(s_{n+1}\). It follows
      that \[ |s - s_n| \leq |s_{n+1} - s_n| = b_{n+1}\text{.} \]
    \end{proof}

  \dbullet{11.6.1 (Absolute Convergence)}
    A series \(\sum a_n\) is called "absolutely convergent" if the series of absolute values \(\sum |a_n|\) is convergent.

  \dbullet{11.6.2 (Conditional Convergence)}
    A series \(\sum a_n\) is called "conditionally convergent" if it is convergent but not absolutely convergent.

  \tbullet{11.6.3 (Absolute Convergence Implication)}
    If a series \(\sum a_n\) is absolutely convergent, then it is convergent.

    \begin{proof}
      Note that \(0 \leq a_n + |a_n| \leq 2|a_n|\). If \(\sum a_n\) is absolutely convergent, then \(\sum |a_n|\) is convergent, so \(\sum 2|a_n|\)
      is convergent. Therefore , by the Comparison Test, \(\sum (a_n + |a_n|)\) is convergent. Then
      \[ \sum a_n = \sum (a_n + |a_n|) - \sum |a_n|\text{.} \] is the difference of two convergent series and is therefore convergent.
    \end{proof}

  \tbullet{11.6.4 (Ratio Test)}
    \begin{enumerate}[i.]
      \item
        If \(\xlimit{n}{\infty}\left|\frac{a_{n+1}}{a_n}\right| = L < 1\), then the series \(\sum_{n=1}^{\infty}\) is absolutely convergent.
      \item
        If \(\xlimit{n}{\infty}\left|\frac{a_{n+1}}{a_n}\right| = L > 1\) or \(\xlimit{n}{\infty}\left|\frac{a_{n+1}}{a_n}\right| = \infty\),
        then the series \(\sum_{n=1}^{\infty} a_n\) is divergent.
      \item
        If \(\xlimit{n}{\infty}\left|\frac{a_{n+1}}{a_n}\right| = 1\), the Ratio test is inconclusive.
    \end{enumerate}

    \begin{proof}
      \begin{enumerate}[i.]
        \item
          Since \(L < 1\), we can choose a number \(r\) such that \(L < r < 1\). Since \[\xlimit{n}{\infty}\left|\frac{a_{n+1}}{a_n}\right| = L \text{ and }
          L < r, \] the ratio \(|a_{n+1}/a_n|\) will eventually be less than \(r\); that is, there exists and integer \(N\) such that
          \[ \left|\frac{a_{n+1}}{a_n}\right| < r, \text{ whenever } n \geq N\text{.} \]
          Putting \(n\) successively equal to \(N, N+1, N+2, \ldots\), we find
          \begin{align*}
            |a_{N+1}| &< |a_N|r \\
            |a_{N+2}| &< |a_{N+1}|r < |a_N|r^2 \\
                      &\cdots \\
            |a_{N+k}| &< |a_N|r^k, \text{ for all } k \geq 1\text{.}
          \end{align*}
          Now the series \( \sum_{k=1}^{\infty}|a_N|r^k \) is a geometric series with \(0 < r < 1\) and must be convergent. Thus the Comparison Test
          tells us that \[ \sum_{n=N+1}^{\infty} |a_n| = \sum_{k=1}^{\infty} |a_{N+k}| \] is also convergent so \(\sum a_n\) is absolutely convergent.
        \item
          If \(|a_{n+1}/a_n| \rightarrow L > 1\) or \(|a_{n+1}/a_n| \rightarrow \infty\), then the ratio \(|a_{n+1}/a_n|\) will eventually be greater
          than \(1\); that is, there exists an integer \(N\) such that \[ \left|\frac{a_{n+1}}{a_n}\right| > 1,\text{ whenever } n \geq N\text{.} \]
          This means that \(|a_{n+1}| > |a_n|\) whenever \(n \geq N\) and so \(\xlimit{n}{\infty} a_n \neq 0\). Therefore \(\sum a_n\) diverges by
          the Test for Divergence.
      \end{enumerate}
    \end{proof}

  \tbullet{11.6.5 (Root Test)}
    \begin{enumerate}[i.]
      \item
        If \(\xlimit{n}{\infty} \sqrt[n]{|a_n|} = L < 1\), then the series \(\sum_{n=1}^{\infty}\) is absolutely convergent.
      \item
        If \(\xlimit{n}{\infty} \sqrt[n]{|a_n|} = L > 1\) or \(\xlimit{n}{\infty} \sqrt[n]{|a_n|} = \infty\), then the series \(\sum_{n=1}^{\infty} a_n\)
        is divergent.
      \item
        If \(\xlimit{n}{\infty} \sqrt[n]{|a_n|} = 1\), the Root Test is inconclusive.
    \end{enumerate}

  \dbullet{11.8.1 (Power Series)}
    A "power series about \(a\)" is a series of the form \[ \sum_{n=0}^{\infty} c_n(x-a)^n = c_0 + c_1(x-a) + c_2(x-a)^2 + \cdots \]
    where \(x\) is a variable and the \(c_n\)'s are constants called the "coefficients" of the series.

  \tbullet{11.8.2 (Convergence of Power Series)}
    For a given power series \(\sum_{n=0}^{\infty} c_n(x-a)^n\), there are only three possibilities:
    \begin{enumerate}[i.]
      \item The series converges only when \(x = a\).
      \item The series converges for all \(x\).
      \item There is a postive number \(R\) such that the series converges if \(|x-a| < R\) and diverges if \(|x-a| > R\).
    \end{enumerate}
    \(R\) in item (iii) is called the "radius of convergence," which is \(0\) in case (i) and \(\infty\) in case (ii) by convention.

  \tbullet{11.9.1 (Term-by-Term Differentiation and Integration)}
    If the power series \(\sum c_n(x-a)^n\) has radius of convergence \(R > 0\), then the function \(f\) defined by
    \[ f(x) = c_0 + c_1(x-a) + c_2(x-a)^2 + \cdots = \sum_{n=0}^{\infty} c_n(x-a)^n \] is differentiable (and therefore continuous)
    on the interval \((a-R, a+R)\) and
    \begin{enumerate}[i.]
      \item \(f'(x) = c_1 + 2c_2(x-a) + 3c_3(x-a)^2 + \cdots = \sum_{n=1}^{\infty} nc_n(x-a)^{n-1}\).
      \item \(\int f(x)dx = C + c_0(x-a) + c_1\frac{(x-a)^2}{2} + c_2\frac{(x-a)^3}{3} + \cdots = C + \sum_{n=0}^{\infty}c_n\frac{(x-a)^{n+1}}{n+1}\).
    \end{enumerate}
    The radii of convergence (though not necessarily the interval of convergence) of the power series in (i) and (ii) are both \(R\).

  \tbullet{11.10.1 (Taylor Series)}
    If \(f\) has a power series representation (expansion) at \(a\), that is, if
    \[ f(x) = \sum_{n=0}^{\infty} c_n(x-a)^n,\quad |x-a| < R \]
    then its coefficients are given by the formula \[ c_n = \frac{f^{(n)}(a)}{n!}\text{.} \]
    This is called the "Taylor series of the function about \(a\)." If \(a = 0\), we call the series
    the "Maclaurin series."

    \begin{proof}
      Suppose \(f\) has a power series representation, then
      \[ f(x) = c_0 + c_1(x-a) + c_2(x-a)^2 + \cdots,\quad |x-a| < R \]
      for radius of convergence \(R\) and coefficients \(c_n\) to be determined.
      We note though that all coefficients can be found by differentiating via Theorem 11.9.1 as follows:
      \begin{align*}
        f'(x) &= c_1 + 2c_2(x-a) + 3c_3(x-a)^2 + \cdots,\quad |x-a| < R \\
        f''(x) &= 2c_2 + (2\cdot 3)c_3(x-a) + (3\cdot 4)c_4(x-a)^2 + \cdots,\quad |x-a| < R \\
              &\cdots
      \end{align*}
      and evaluating the \(\spscript{n}{th}\) derivative at \(a\) to get the \(\spscript{n}{th}\) coefficient.
    \end{proof}

  \dbullet{11.10.2 (\(\spscript{n}{th}\)-Degree Taylor Polynomials)}
    The partial sum \[T_n(x) = \sum_{i=0}^{n}\frac{f^{(i)}(a)}{i!}(x-a)^i\] is the "\(\spscript{n}{th}\)-degree Taylor polynomial of \(f\) at \(a\)."

  \tbullet{11.10.3}
    If \(f(x) = T_n(x) + R_n(x)\), where \(T_n\) is the \(\spscript{n}{th}\) Taylor polynomial of \(f\) at \(a\) and \(R_n\) is the "remainder"
    of the Taylor series such that \[\xlimit{n}{\infty}R_n(x) = 0\] for \(|x-a| < R\), then \(f\) is equal to the sum of its Taylor series on the
    interval \(|x-a| < R\).

    \begin{proof}
      In general, \(f(x)\) is the sum of its Taylor series if \(f(x) = \xlimit{n}{\infty}T_n(x)\). If we let
      \[ R_n(x) = f(x) - T_n(x)\text{ so that }f(x) = T_n(x) + R_n(x)\text{,} \] and we can show \(\xlimit{n}{\infty} R_n(x) = 0\),
      then it follows that \[ \xlimit{n}{\infty}T_n(x) = \xlimit{n}{\infty}[f(x) - R_n(x)] = f(x) - \xlimit{n}{\infty}R_n(x) = f(x)\text{.} \]
    \end{proof}

  \pagebreak
  \tbullet{11.10.4 (Taylor's Inequality)}
    If \(|f^{(n+1)}(x)| \leq M\) for \(|x-a| \leq d\), then the remainder \(R_n(x)\) of the Taylor series satisfies the inequality
    \[ |R_n(x)| \leq \frac{M}{(n+1)!}|x-a|^{n+1}\text{ for } |x-a| \leq d\text{.} \]

    \begin{proof}
      Let \(n = k\) for some \(k \in \bbz^{+}\). By assumption, we have \(|f^{(k+1)}(x)| \leq M\) for \(|x-a| \leq d\). In particular,
      we have \(f^{(k+1)}(x) \leq M\), so for \(a \leq x \leq a + d\) we have \[ \int_a^x f^{(k+1)}dt \leq \int_a^x M dt\text{.} \]
      By part (ii) of the Fundamental Theorem of Calculus:
      \begin{align*}
        f^{(k)}(x) - f^{(k)}(a) &\leq M(x-a) \\
        f^{(k)}(x) &\leq M(x-a) + f^{(k)}(a) \\
        \int_a^x f^{(k)} dt &\leq \int_a^x [M(x-a) + f^{(k)}(a)] dt \\
        f^{(k-1)}(x) - f^{(k-1)}(a) &\leq \frac{M}{2}(x-a)^2 + f^{(k)}(a)(x-a) \\
        f^{(k-1)}(x) &\leq \frac{M}{2}(x-a)^2 + f^{(k)}(a)(x-a) + f^{(k-1)}(a) \\
                     &\cdots
      \end{align*}
      This process completes when we have \(f(x)\) on the left hand side. At this point, we'll have
      \[ f(x) \leq \frac{M}{(n+1)!}|x-a|^{n+1} + T_n(x)\text{.} \] Therefore,
      \[ f(x) - T_n(x) = R_n(x) \leq \frac{M}{(n+1)!}|x-a|^{n+1}\text{.} \]
    \end{proof}

\end{outline}

\end{document}
